---
title: "Thinking about Factor Analysis"
author:  |
  <div class="title"><span style="font-size:125%; font-variant:small-caps; font-style:normal; ">Michael Clark</span><br>
  <!-- <img src="img/signature-acronym.png" style="width:33%; padding:10px 0;"> <br> -->
  <!-- <img src="img/ARC-acronym-signature.png" style="width:22%; padding:10px 0;"> </div> -->
date: "`r Sys.Date()`"
output: 
  html_document: 
    css: standard_html_fa.css
    df_print: kable
    highlight: pygments
    theme: sandstone
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message = FALSE, warning=FALSE, R.options=list(width=120), 
                      cache.rebuild=F, cache=TRUE,
                      fig.align='center', dev = 'svg', dev.args=list(bg = 'transparent'))
knitr::opts_knit$set(root.dir = '../')
library(tidyverse); library(htmltools); library(lazerhawk); library(heatR); library(pander); library(lavaan);
```


## Introduction


Key ideas

- Dimension Reduction
- Matrix Factorization
- Latent Linear Models
- Measurement error


Each section will provide some suggested packages to serve as starting points for standard implementations.  Just note that there will likely be several packages providing enhancements in a variety of fashion.


## PCA

Seek to find orthogonal factors to minimize reconstruction error.

$$\mathscr{Loss} = X - ZW^T$$

X is the $NxD$ matrix of $N$ observations of $D$ variables, $Z$ are the $N x L$ component scores where L < D, while $W$ are the $DxL$ weights, typically referred to as the <span class="emph">factor loading</span> matrix.

Each component, or factor, created accounts for less of the variance in the original data.  With all components, 

$$X = ZW^T$$

The MNIST data contains 28 by 28 pixel images of digits 0-9.  If we unroll the data to 784 columns, each row represents a single digit.  We can see in the following how well we can reconstruct a digit via PCA.  Even with only two components we can get a sense of what we're looking at.  With all components the reconstruction is perfect.

<img src="../img/pca_recon.png" style="display:block; margin: 0 auto;">
<!-- I guess html ignores working directory -->

### Example

Let's see an example with more digestible results.  The following data are 16 multiple choice ability items taken from the Synthetic Aperture Personality Assessment (SAPA) web based personality assessment project. There are 1525 total subjects, and four categories of questions.

- reason: verbal reasoning questions
- letter: In the following alphanumeric series, what letter comes next?
- matrix: A matrix reasoning task
- rotate: spatial rotation task

```{r data_cogab, echo=1:2}
library(psych)
cog_ability = psych::iqitems
DT::datatable(cog_ability[1:10,],
              rownames = F,
              options = list(dom='t', 
                             scrollX=T,
                             autoWidth=T,
                             columnDefs=list(list(width = '100px', targets='_all'),
                                             list(className = 'dt-center', targets = '_all'))),
              width='100%')
```

<br>


Let's do a PCA.  We'll look at four components, though remember that PCA technically returns as many components as there are variables. You're just seeing part of the results. Note that PCA almost universally requires you to <span class="emph">standardize</span> the data,  so that each variable has the same variance.  Otherwise you'll just get components reflecting the relative variances. In what follows the PCA is done on the correlation matrix, which amounts to the same thing.

```{r pc_cog, echo=-3}
pc = principal(cog_ability, 4)
pc
p = ncol(cog_ability)
```

First focus on the portion of the output where it says `SS loadings` . The first line is the sum of the squared loadings[^eigen] for each  component (in this case where we are using a correlation matrix as the basis, summing across all `r p` possible components would equal the value of `r p`). The `Proportion Var` tells us how much of the overall variance the component accounts for out of all the variables (e.g. `r paste(round(print(pc)$Vaccounted[1], 2), '/', p, ' = ', round(print(pc)$Vaccounted[1]/p, 2))`).  The `Cumulative Var` tells us that `r pc$factors` components make up over `r round(sum(colSums(pc$loadings^2))/p, 2)*100`% the variance. The others are the same thing just based on the `r pc$factors` retained components rather than all `r p` variables (i.e. the cumulative explained variance would = 1). We can see that the second component accounts for less variance, and this would continue with additional components, where each accounts for a decreasing amount of variance.

Some explanation of the other parts of the output:

- `h2`: the amount of variance in the item/variable explained by the (retained) components. It is the sum of the squared loadings, a.k.a. <span class="emph">communality</span>. For example, the first reasoning item shares only 38% of its variance with these components.
- `u2`: 1 - h2
- `com`: A measure of complexity. A value of 1 might be seen for something that loaded on only one component, and zero otherwise (a.k.a. perfect simple structure).


<span class='emph'>Loadings</span>, also referred to as the <span class="emph">pattern matrix</span>, in this scenario represent the estimated correlation of an item with its component, and provide the key way in which we interpret the factors. As an example, we can reproduce the loadings by correlating the observed variables with the estimated component scores[^rotated].

```{r pc_LoadingsReproduced, dependson=-1}
cor(cog_ability, pc$scores, use = 'pair') %>% round(2)
```


It can be difficult to sort it out just by looking at the values, so we'll look at it visually. In the following plot, stronger loadings are indicated by blue, and we can see the different variables associated with different components. 

```{r pc_LoadingsVis, echo=F, cache=FALSE}
library(htmltools)
# tags$div(style="width:50%; margin-left:auto;  margin-right:auto",
#          d3heatmap::d3heatmap(pc$loadings, dendrogram='none',
#                               colors=scales::col_bin(RColorBrewer::brewer.pal(11, "RdBu"), 
#                                                      domain=c(-.3,1), bins=50), 
#                               width=500)
#          )
tags$div(style="width:50%; margin-left:auto;  margin-right:auto",
heatmaply::heatmaply(pc$loadings, dendrogram='none',
                     colors=scales::col_bin(RColorBrewer::brewer.pal(11, "RdBu"), domain=c(-1,1), bins=50)(sort(pc$loadings)),
                     hide_colorbar=T, 
                     plot_method='ggplot'
                     ) %>% 
  plotly::layout(yaxis = list(showline = F),
         xaxis = list(showline = F)
         )
, width=500
)
```

<br>

Many would use what's called a <span class="emph">biplot</span>, which shows the components on the axes, the component scores of the data, and the loadings as vectors whose length and direction specify the strength of the loading on a particular component.

```{r biplot, echo=FALSE}
# spider(1:8, data=cor(cog_ability[,1:8], use='pair'), cex.lab=.5, scale=1)
psych::biplot.psych(pc, choose = 1:2, 
                    col = viridis::viridis(3, alpha = c(.1,.75)), 
                    main = 'PCA Biplot',
                    hist.col = '#00aaff80',
                    bty='n')
```

Biplots take a bit to figure out just for two components, and one can't really get a sense of more than two at time. However, using a graphical model makes clear what's going on for all retained components. Low loadings have been suppressed for additional clarity.

```{r pca_graph, echo=FALSE}
# fa.graph(pc, out.file = 'FA_workshop/pc_cog_graph.gv',
#          node.font = 'Roboto',
#          edge.font = 'Roboto',
#          rank.direction = 'TB',
#          main='PCA Results',
#          fontize=10,
#          bgcolor=transparent,
#          arrowhead=none)
tags$div(style="width:50%; margin-left:auto;  margin-right:auto; font-size:50%",
         DiagrammeR::grViz('FA_workshop/pc_cog_graph.gv', width='100%', height='600px')
)
br()
```


Interpretation is the fun, but commonly difficult, part. In this case, the variables don't appear to be grouping like we'd expect, except for some of the reasoning scores[^badfactor].  It's worth mentioning the <span class="emph">naming fallacy</span> at this point.  Just because we associate a factor with some concept, doesn't make it so. In addition, for PCA the goal is not interpretation, but to reduce the data while retaining as much of the variance as possible.


### PCA Summary

Why do it? 

- Reduce the dimensions of the data
- Retain as much variance as possible

Issues

- It might not be the best choice with some data types
- It is not the choice to make if you are wanting to understand the *co*variance in the data (though it will often agree with methods that do)
- Non-numeric or mixed data types


Other stuff

- Scale the variables
- Biplots are ugly

#### Suggested R packages

- psych
- pcaMethods (Biocondudctor)

## FA

Now let's examine what is sometimes called *common* factor analysis, and also sometimes *exploratory* factor analysis in the social sciences, and even 'classical' or 'traditional', but typically just <span class="emph">factor analysis</span> (FA) everywhere else.  While we can use both PCA and FA for similar reasons (dimension reduction) and even have similar interpretations (in terms of loadings), there are some underlying subtleties between the two that provide unique distinctions.  Noting these distinctions with some detail will require some matrix notation, but for readers not so keen on such presentation they may note the images and concluding points.

$$X \approx ZW^T$$

First let's revisit PCA, where we can depict it conceptually as an approach where we attempt to approximate the correlation matrix in terms of the product of components, represented by our loading matrix $L$.

$$R = LL'$$
and

$$C = XW$$

In other words, each component *score* $C$, i.e. the score for a particular observation with regard to the component, is a weighted combination of the $p$ observed variables $X$, the weights (with weight/loading matrix $W$) of which are determined by the loadings, but yet do not say anything about the correlations between the variables.

Things are different with factor analysis. Now the causal flow is in the other direction, originating with the latent variables. 

$$X \approx FW$$

Each observed variable $x$ is a function of the latent variables that it is associated with.  In addition, we also take into account the uniquenesses $\Psi$, or that part which the factors do not explain.


And to reproduce the correlation matrix:

$$R \approx LL'$$
$$R = LL' + \Psi$$

So if we just use the loadings from the FA, we cannot reproduce the correlation matrix exactly, we need to add the uniquenesses as well.

What this amounts to conceptually are a few key ideas:

- Factor analysis focuses on covariance. PCA focuses on variance.
- Factors are the cause of the observed variables, variables are the cause of components.
- Factor analysis does not assume perfect measurement of observed variables.


```{r fa_res}
fa_model = fa(cog_ability, 4, rotate='oblimin')
fa_model
```



```{r fa_LoadingsVis, echo=F, cache=FALSE}
tags$div(style="width:50%; margin-left:auto;  margin-right:auto",
heatmaply::heatmaply(fa_model$loadings, dendrogram='none',
                     colors=scales::col_bin(RColorBrewer::brewer.pal(11, "RdBu"), domain=c(-1,1), bins=50)(sort(pc$loadings)),
                     hide_colorbar=T, 
                     plot_method='ggplot'
                     ) %>% 
  plotly::layout(yaxis = list(showline = F),
         xaxis = list(showline = F)
         )
, width=500
)
```




## FA for measurement

### "Confirmatory" Factor Analysis

Some needlessly distinguish between <span class="emph">exploratory</span> factor analysis and <span class="emph">confirmatory</span> factor analysis, but all the latter is doing is constraining some loadings to be zero.  Otherwise they are identical techniques.

Why would you set loadings to zero? Theory would suggest it.  For example if we truly to be four latent constructs in this cognitive ability data, we can set things up to 'confirm' this.


```{r eval=FALSE}
modelCode = "
  verbal =~ reason.4 + reason.16 + reason.17 + reason.19
  spatial =~ rotate.3 + rotate.4 + rotate.6 + rotate.8

  # not necessary
  # verbal ~~ spatial

  # if you don't want them correlated
  # verbal ~~ 0*spatial
"

famod = cfa(modelCode, data=cog_ability)
summary(famod, standardized=T, rsq=T, nd=2)
pars = parameterEstimates(famod, standardized=T)
```


#### Suggested R packages

- psych
- lavaan

## As latent linear models


### FA

$$ X \sim \mathcal{N}(ZW^T + \mu, \Psi) $$
$\mu$ are the intercepts, $\Psi$ is a $DxD$ covariance matrix.


### Probabilistic PCA

For probabilistic PCA $\Psi$ is $\sigma^2I$.

### PCA

For standard PCA, $\sigma \rightarrow 0$,

## More techniques




### Count-based Matrix Factorization

Commonly our data regards counts or otherwise compositional data.  In this case we can use techniques that are better suited to such data.
 
#### NMF

Non-negative matrix factorization is similar to PCA, just with the constraint that scores and weights be positive.  It is (in the usual implementation) identical to probabilistic latent semantic analysis.

<img src="../img/nmf_recon10.png" style="display:block; margin: 0 auto;">

#### LDA

<span class="emph">Latent Dirichlet Allocation</span> takes a different approach to deal with count data. The classic example regards text analysis, where LDA is applied to word frequencies across documents.  Consider a <span class="emph">document-term</span> matrix where each row regards a document, and each column a word or term.  LDA looks for latent *topics* that are probability distributions over the *terms*.    There is a probability distribution for the topics as well as the terms, and given both, we will see some occurrence of the term in each document. Each *document* can be seen a mixture of topics, though some will be more probable than others.

In the factor analysis sense, each variable (the term) will have some probability of occurring for a given topic, i.e. latent factor. One can think of the term/variable probabilities for a specific topic similar to how we did loadings in the factor analysis sense.  Every variable will have some non-zero probability of occurrence for a given factor, but often they are essentially zero. Factor scores are the document topic probabilities.


Given the probabilities of topics and terms within topics, there is a multinomial draw of counts for an observation.  With this probabilistic result, there isn't a 'reconstruction' connotation with LDA.  However we can simulate a mulinomial draw based on the total counts we see.  For example, let's assume 5 terms seen a total of 20 times given some probability.

```{r multinom_ex, echo=-1}
set.seed(123)
t(rmultinom(1, size = 20, prob=c(.5, .2, .15, .1, .05))) # first term is most likely, 5th term is least likely
```

Those are the counts we see for a given observation. 


LDA wouldn't be the best tool if your goal is prediction, and you don't care much about interpretation.


<img src="../img/lda_recon.png" style="display:block; margin: 0 auto;">


#### Suggested R packages

- NMF
- lda
- quanteda
- text2vec



### SEM

<span class="emph">Structural Equation Modeling</span> is a very general technique that allows one to incorporate latent variables and observed variables within a single modeling framework, allowing for indirect effects and multiple outcomes.  We actually already did an SEM when we did the confirmatory factor analysis.

```{r sem_graph, echo=FALSE}
# the sigh that comes with this can be heard round the world.  Diagrammer can't
# read the file with the syntax, but can read object with the same syntax

test = "digraph test {
 graph [rankdir=TB bgcolor=transparent splines=false]
 
 #### Node statements
 ## Intialization of node attributes
 node [style=filled, color=gray75,  fontcolor=gray50];
 
 ## latent variables and associated variances
    SES [shape=circle, fontsize=10, width=1.25, height=1, fillcolor='#1b4d9e', fontcolor=white]; 
    SESvar[shape='none' color='none' fillcolor='none' label='' width=0];


 subgraph {
    rank = same;
    Alien67 [shape=circle, fontsize=10, width=1.25, height=1, fillcolor='#9e911b', fontcolor=white];
    Alien71 [shape=circle, fontsize=10, width=1.25, height=1, fillcolor='#9E1B34', fontcolor=white];
 }

 subgraph {
    rank = same;
    Alien67;
    Alien67var[shape='none' color='none' fillcolor='none' label='', width=0];
    Alien71var[shape='none' color='none' fillcolor='none' label='', width=0];
 }
 #### Edge statements
 
 ## Initialization of edge attributes

 edge [color = gray80 rel = yields dir=forward fontsize=12  penwidth=4 len=2]
 
 ## latent to latent

 edge [label='' fontcolor=gray25 minlen=1 ] # set min to 1 made it centered
 SES -> Alien67 [label='-.56' fontcolor='#8B00008F' color='#8B00008F' alpha=.5];   # the alpha attribute is utterly ignored for the edge so use alpha func; still will be ignored for fontcolor
 SES -> Alien71 [label='-.21' fontcolor='#8B000036' color='#8B000036'];

 edge [label='' fontcolor=gray25 dir=forward ]
 Alien67 -> Alien71 [label='.57' fontcolor='#00640091' color='#00640091' minlen=3 ];


 ## variances

 edge [penwidth=1 len=1]
 SESvar -> SES[label='&nbsp; 1.0'];
 Alien67var -> Alien67[label='.68'];
 Alien71 -> Alien71var[label='.50' dir=back];

 }

}
 "

# DiagrammeR::grViz(test, width='100%', height='400px')
tags$div(style="width:100%; margin:auto auto; ",
DiagrammeR::grViz(test, width='100%', height='400px')
)
```

#### Suggested R packages

- lavaan
- semTools

### IRT

Item response theory can be seen as latent variable modeling for categorical data.  It is especially applied to testing environments where the items are binary correct vs. not outcomes.  An underlying logistic model relates the latent variable, which can be seen as 'ability' in the testing context, to the observed items.  The conceptual latent variable model is identical to confirmatory factor analysis.

#### Suggested R packages
- ltm

### ICA

The latent linear model versions of PCA and factor analysis assume the observed variables are normally distributed (even standard PCA won't work nearly as well if the data aren't.  This is not required, and ICA, <span class="emph">independent components analysis</span> does not. This visualization duplicates that seen in Murphy (2012) where we have two (uniform) independent sources.  We can see that the ICA correctly recovers those components.

```{r ica, echo=FALSE, results='hide', cache=FALSE}
set.seed(2)
N = 1000

A = matrix(c(2,3,2,1), 2)*.3
Suni = matrix(runif(2*N, min = -1), ncol = 2)*sqrt(3)
Xuni = Suni %*% A

ica_res = fastICA::fastICA(Xuni, 2)
ica_res$A
A

pc_res = psych::principal(Xuni, 2)$scores 
```

```{r ica_plot, echo=FALSE, cache=FALSE}
p1 = qplot(Suni[,1], Suni[,2], color=I('#ff550040')) + 
  labs(x='', y='', title='Sources') +
  lims(x=c(-3,3), y=c(-3,3)) +
  theme_trueMinimal()
p2 = qplot(Xuni[,1], Xuni[,2], color=I('#ff550040')) +
  lims(x=c(-3,3), y=c(-3,3)) +
  labs(x='', y='', title='Observed Data') +
  theme_trueMinimal()
p3 = qplot(pc_res[,1], pc_res[,2], color=I('#ff550040')) +
  lims(x=c(-3,3), y=c(-3,3)) +
  labs(x='', y='', title='PCA') +
  scale_x_reverse() +
  theme_trueMinimal()
p4 = qplot(ica_res$S[,1], ica_res$S[,2], color=I('#ff550040')) + 
  labs(x='', y='', title='ICA') +
  lims(x=c(-3,3), y=c(-3,3)) +
  theme_trueMinimal()
cowplot::plot_grid(p1, p2, p3, p4)
```


```{matlab ica_matlab, echo=F, eval=F}
setSeed(2);
%Number of data points
N=100;
%Choose a nice mixing matrix
A=[2,3;2,1]*.3;


%% Uniform data

%Create data with uniform distribution
Suni=(rand(2,N)*2-1)*sqrt(3);
Xuni=A*Suni;
Vuni = fastica(Xuni,'only','white');
Shat = fastica(Xuni,'g','tanh','approach','symm');
cmax=3; 

h1=plot(Suni(1,:),Suni(2,:),'.',[-cmax,cmax],[0,0],'k',[0,0],[-cmax,cmax],'k'); 
%just cosmetics:
set(h1,'LineWidth',2);
set(h1,'MarkerSize',16);
axis equal;
title('uniform data')
printPmtkFigure('icaUniformSource')

%Plot uniform mixed data
figure;
h1=plot(Xuni(1,:),Xuni(2,:),'.',[-cmax,cmax],[0,0],'k',[0,0],[-cmax,cmax],'k'); 
axis equal;
set(h1,'LineWidth',2);
set(h1,'MarkerSize',16);
title('uniform data after linear mixing')
printPmtkFigure('icaUniformMixed')


%Plot uniformly distributed data after PCA whitening
figure;
h1=plot(Vuni(1,:),Vuni(2,:),'.',[-cmax,cmax],[0,0],'k',[0,0],[-cmax,cmax],'k'); 
axis equal;
set(h1,'LineWidth',2);
set(h1,'MarkerSize',16);
title('PCA applied to mixed data from uniform source')
printPmtkFigure('icaUniformPCA')


%Plot estimated ICs from uniformly distributed data
figure
h1=plot(Shat(1,:),Shat(2,:),'.',[-cmax,cmax],[0,0],'k',[0,0],[-cmax,cmax],'k'); 
```


Why do it? You believe that truly independent sources of signal underly your data, and that those sources are not normally distributed. You deal with images or sound[^deepimage].

#### Suggested R packages

- fastICA


### Multidimensional Scaling

Multidimensional scaling[^mds] is a technique for converting a matrix of observation dissimilarities into a low-dimensional map that preserves those distances as well as possible.

The first step in any MDS is choosing a similarity measure. Often this will be a metric or distance, i.e.:
- Euclidean Distance for continuous variables
- Manhattan distance or Jacaard dissimilarity for binary variables
- Similarity measures can often become dissimilarity measures by inverting (x → 1/x) or subtracting from 1 (x → 1−x).  

The basic idea is to come up with what we might call components or factors that minimize the difference in factor scores for two observations from the actual distance.

```{r mds, echo=1:2}
data(UScitiesD)
UScitiesMDS <- cmdscale(UScitiesD)
mat <- as.matrix(UScitiesD)
colnames(mat) <- c('Atl','Chi','Den','Hou','LA','Mia','NY','SF','Sea','DC')
knitr::kable(mat,format='html') %>% kableExtra::kable_styling(bootstrap_options=c('striped','bordered'),font_size=14)
```

<br>

We can use MDS to obtain a 2-dimensional map preserving distances as well as possible.

```{r mds_plot, echo=FALSE}
(-1*UScitiesMDS) %>% 
  as_data_frame() %>% 
  rename(`Factor 1` = V1, `Factor 2` = V2) %>% 
  mutate(city=attr(UScitiesD,'Labels')) %>% 
  ggplot(aes(x=`Factor 1`, y=`Factor 2`)) +
  geom_text(aes(label=city)) +
  lims(x=c(-1750,1250)) +
  theme_trueMinimal()
```

#### Suggested R packages

- Base R 

### Collaborative Filtering

#### Suggested R packages
- recommenderlab

### Time-based

Hidden markov models can be used to model latent discrete states. In terms of a graphical model, we can depcit it as follows:

```{r hmm_graph, echo=FALSE}
hmm = "
digraph Factor  {
  
 graph [rankdir=TB  bgcolor=transparent]
 node [fontname='Roboto' fontsize=10 shape=circle, width=.5];
   z1  [label = <z<sub>1</sub>>];
   z2  [label = <z<sub>2</sub>>];
   z3  [label = <z<sub>3</sub>>];

   # because diagrammer can't maintain order  ; 
   x1  [label = ☻];
   x2  [label = ♥];
   x3  [label = ☺];
   x4  [label = §];

 node [shape=box width=1];
 edge [fontname='Roboto' fontsize=10 minlen=5 penwidth=2 color='#00aaff'];
  z1 -> z2 [label = ''  arrowhead=''];
  z1 -> z3 [label = ''  arrowhead='' penwidth= 1];
  z2 ->  z2 [label = '' arrowhead='' penwidth=1.5];
  z2 ->  z3 [label = '' arrowhead='' penwidth=.5];
  z3 ->  z1 [label = '' arrowhead='' penwidth=2];


 edge [fontname='Roboto' fontsize=10 minlen=1 penwidth=.5 color='#ff5500'];
  z1 ->  x1 [label = '' arrowhead='' penwidth=.05];
  z1 ->  x2 [label = '' arrowhead='' penwidth=.25];
  z1 ->  x3 [label = '' arrowhead='' penwidth=.45];
  z1 ->  x4 [label = '' arrowhead='' penwidth=.25];

  z2 ->  x1 [label = '' arrowhead='' penwidth=.25];
  z2 ->  x2 [label = '' arrowhead='' penwidth=.05];
  z2 ->  x3 [label = '' arrowhead='' penwidth=.15];
  z2 ->  x4 [label = '' arrowhead='' penwidth=.50];

  z3 ->  x1 [label = '' arrowhead='' penwidth=.5];
  z3 ->  x2 [label = '' arrowhead='' penwidth=.15];
  z3 ->  x3 [label = '' arrowhead='' penwidth=.15];
  z3 ->  x4 [label = '' arrowhead='' penwidth=.20];




 { rank=same;
  z1; z2; z3; }
 { rank=same;
  x1; x2; x3; x4;}

}
"

DiagrammeR::grViz(hmm, width='100%', height='400px')
# tags$div(style="width:100%; margin:auto auto; ",
# DiagrammeR::grViz(hmm, width='100%', height='400px')
# )
```

In this sense we have latent variable $z$ that represents the hidden state of a system, while the outcome is what we actually observe.  There are three latent states, and the relative width of the edge reflects the <span class="emph">transition probability</span> of moving from one state to the other.  The $x$ are the categories of observations we can potentially observe, and there is some probability, given a latent state of seeing a particular category.  Such a situation might lead to the following observed sequence of observations:

We start in state 1 where the heart and yellow smiley are most probable. Let's say we observe ♥.  The second state is most likely so we get there, and because it has some probability of staying in that state, we observe § and §. We finally get to state 3 and see <span class="" style='color:black'>☻</span>, where we go back to state 1, where we see ☺, jump to latent state 3 etc.  And such a process continues for the length of sequence we see.

This can be seen as a mixture model/latent class situation that we'll talk about more later.  The outcomes could also be continuous, such that the latent state determines the likelihood of the observation in a manner more directly akin to the latent linear model for standard factor analysis. 


#### Suggested R packages

- <span class="pack">msm</span>
- <span class="pack">HMM</span>
- <span class="pack">HiddenMarkov</span>





## Latent classes

Take a look at the following data. It regards the waiting time between eruptions and the duration of the eruption (both in minutes) for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA.

```{r faithfulPlot, echo=F}
data("faithful")
library(plotly)
plot_ly(data=faithful, x=~waiting, y=~eruptions) %>% 
  theme_plotly()
```

In some contexts, possibly way too many, some assume the latent variables are discrete in nature rather than continuous.  In this case, the dimension reduction occurs along the *rows* rather than the columns.  We'll talk about two types.
 
### Mixture models

A basic approach for categorical latent variable analysis from a model based perspective[^cluster] could be seen as follows:

1. Posit the number of clusters you believe there to be
2. For each observation, estimate those probability of coming from either cluster assuming some underlying distribution
3. Assign observations to the most likely class (i.e. the one with the highest probability)

Typical methods are assuming an underlying normal distribution, such that each observation is the weighted some of its likelihood under the parameters for each proposed cluster with their respective estimated parameters.  For example with two clusters, each with an estimated mean and variance, we get the likelihood for a particular observation for each cluster.  We weight them by its estimated probability of membership in that cluster.

As an example, we'll use <span class="pack">mclust</span> on the old faithful data, positing 2 clusters.

```{r clus_mclust, echo=1:3}
library(mclust)
mod = Mclust(faithful, G = 2)
summary(mod)

par(mfrow=c(1,2))
plot(mod, what='classification')
plot(mod, what='uncertainty')
graphics::layout(1)
```

We can see that things break out more or less as we might expect, though there is a bit more uncertainty with observations in the middle.


The nice thing about model based approaches is that if we want to compare different solutions we can use the same tools we do in regression settings.  Here, mclust does a search over many models (not just differences in cluster sizes, but also things like equal variances, shape, etc.)  and selects the 'best' model according to BIC.

```{r clus_mclust2, echo=1}
mod = Mclust(faithful, verbose=F)
par(mfrow=c(1,2))
plot(mod, what='BIC')
plot(mod, what='classification')
graphics::layout(1)
```



#### For regression

Mixture models are often used in the regression setting, similar to the latent class setting that we've been describing.  One approach regards the outcome of interest, which you think has latent categories, and you model it as such. In a standard regression setting you would get two sets of output, one for each class, along with estimated probabilities of class membership for each observation.


```{r flexmixEruptions_workshop, echo=-1}
set.seed(1234)
library(flexmix)
mod = flexmix(eruptions~waiting,  data=faithful, k = 2)
summary(mod)
```

We can see from the summary about 2/3 are classified to one group. We also get the estimated means and standard deviations for each group, as well as note respective probabilities of each observation for each class.  Note that the group labels are completely arbitrary.

```{r flexmixEruptions2_workshop, echo=-2}
parameters(mod)    # means (Intercept) and std dev (sigma) for each group
head(mod@posterior$scaled, 10) %>% round(3) %>% data.frame() %>% pander::pander()  # show some estimated probabilities
```

<br>

The key idea is as before, that the response is from two distributions, e.g. two normal distributions, each with their own mean, modeled by covariates, and some variance.  This very commonly applied in cases where there are many zeros in the data, leading to <span class="emph">zero-inflated</span>, <span class="emph">hurdle</span> and similar models, where the zeros are modeled by a logistic regression and the counts by Poisson or negative binomial, but the approach is much more general.


 
### "Cluster analysis"

Aside from mixture models, when people use the term 'cluster analysis' they are typically referring to distance-based methods, where given a distance matrix that informs how dissimilar observations are from one another, the methods try to create clusters of observations that are similar to one nother.



#### K-means

<span class="emph">K-means</span>K-means cluster analysis is hands-down the most commonly used clustering method.  Conceptually it's fairly straightforward- find $k$ clusters that minimize the variance of its members from the mean of its members.  As such it's easy to implement in standard data settings.

K-means can actually be seen as a special case of the Gaussian mixture model above (similar to EII in the BIC plot, which is an attempt to find spherical clusters of equal size and shape).  It also has connections to PCA and ICA. The general issue is tryin


```{r kmeans, out.width='90%'}
clus2 = factor(kmeans(faithful, 2)$cluster)
c2 = ggplot(aes(x=waiting, y=eruptions), data=faithful) +
  geom_point(aes(color=clus2)) +
  theme_trueMinimal()

clus3 = factor(kmeans(faithful, 3)$cluster)
c3 = ggplot(aes(x=waiting, y=eruptions), data=faithful) +
  geom_point(aes(color=clus3)) +
  theme_trueMinimal()
cowplot::plot_grid(c2, c3)
```

#### Hierarchical

Other methods can be thought of as a clustering the data in a hierarchical.  These can start at the bottom of the hierarchy (<span class="emph">agglomerative</span>), allowing every observation into its own cluster, and successively combining them. For example, first choose a measure of dissimilarity, and combine the two observations that are most alike, then add one to those or if another pair are closer, make a new cluster.  Conversely, one can start with every observation in one cluster, and take the most dissimilar and split it off, continuing on until every observation is in its own cluster.

```{r hclust_dendrogram, echo=FALSE}
heatmaply::heatmaply(iris[,1:4], Colv=F, labRow=NA, showticklabels=F, hide_colorbar=T, colors=viridis::plasma)
```

Practically every decision one has to make (distance, linkage method, cluster determination, general approach) is arbitrary.  While these are actually still not uncommonly used, you always have better alternatives.  They are fine to use in a quick visualization to sort the data more meaningfully though, as above.
 
### Predictive Modeling

### Issues

#### Model Selection

Non-model based methods provide no non-arbitrary way of selecting the number of clusters within a particular method, or comparing different approaches to the same solution.  Many have been offered and in typical settings they will rarely agree.

#### Latent Expressivity

You are never going to have it recommended that you take continuous outcomes or predictors and categorize them.  It is sometimes done as a computational convenience, but if you're interested in interpretability and inference, it's almost always the wrong choice.

Now ask, why would you do it with latent variables?  Unless there is very strong theory or an actual physical aspect involved (e.g. )

#### Suggested R packages

- mclust
- flexmix
- kmeanspp
- Base R

## Other

 - ICA
 - Recommender systems
 - Time-based (e.g. HMM)
 - Bayesian
 
 
## Issues

- how many latent factors/classes/
- Interpretation
- two step models vs. simultaneous estimation for prediction



[^eigen]: They are the <span class="emph">eigenvalues</span> of the correlation matrix.  In addition, they are the diagonal of the crossproduct of the loading matrix.

[^rotated]: These results have been rotated, something practically no one using PCA actually does.  

[^badfactor]: Results like this are more the rule than the exception in my experience. Not enough initial development is typically done with scales, and when other people use them in other settings, it often ends with disappointment.  Just think, many psychological scales are developed on freshman psychology students.  How well do you think that will generalize?

[^pca_normal]: even standard PCA won't work nearly as well if the data aren't.  This is not required, and ICA, <span class="emph">independent components analysis</span> does not.<span class="marginnote">The visualization duplicates that seen in Murphy (2012).

[^mds]: This section has been gracefully stolen from [James Henderson](https://github.com/jbhender/Workshops/).

[^deepimage]: These days one would probably be more likely to use deep learning techniques for audio and images, at least if prediction was the primary concern.