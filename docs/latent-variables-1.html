<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Graphical and Latent Variable Modeling</title>
  <meta name="description" content="Structural Equation Modeling: This document focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. It is has special emphasis on the lavaan package. It will continue to be a work in progress, particularly the sections after the SEM chapter. Topics include: graphical models, including path analysis, bayesian networks, and network analysis, mediation, moderation, latent variable models, including principal components analysis and ‘factor analysis’, measurement models, structural equation models, mixture models, growth curves. Topics I hope to provide overviews of in the future include other latent variable techniques and extensions such as IRT, collaborative filtering/recommender systems, hidden markov models, multi-group models etc.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Graphical and Latent Variable Modeling" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://m-clark.github.io/docs/sem/" />
  <meta property="og:image" content="https://m-clark.github.io/docs/sem/img/198R_1.png" />
  <meta property="og:description" content="Structural Equation Modeling: This document focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. It is has special emphasis on the lavaan package. It will continue to be a work in progress, particularly the sections after the SEM chapter. Topics include: graphical models, including path analysis, bayesian networks, and network analysis, mediation, moderation, latent variable models, including principal components analysis and ‘factor analysis’, measurement models, structural equation models, mixture models, growth curves. Topics I hope to provide overviews of in the future include other latent variable techniques and extensions such as IRT, collaborative filtering/recommender systems, hidden markov models, multi-group models etc." />
  <meta name="github-repo" content="m-clark/sem/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Graphical and Latent Variable Modeling" />
  
  <meta name="twitter:description" content="Structural Equation Modeling: This document focuses on structural equation modeling. It is conceptually based, and tries to generalize beyond the standard SEM treatment. It is has special emphasis on the lavaan package. It will continue to be a work in progress, particularly the sections after the SEM chapter. Topics include: graphical models, including path analysis, bayesian networks, and network analysis, mediation, moderation, latent variable models, including principal components analysis and ‘factor analysis’, measurement models, structural equation models, mixture models, growth curves. Topics I hope to provide overviews of in the future include other latent variable techniques and extensions such as IRT, collaborative filtering/recommender systems, hidden markov models, multi-group models etc." />
  <meta name="twitter:image" content="https://m-clark.github.io/docs/sem/img/198R_1.png" />



<meta name="date" content="2018-02-24">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="graphical-models-1.html">
<link rel="next" href="structural-equation-modeling.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-0.9.2/grViz.js"></script>
<script src="libs/d3-3.5.3/./d3.min.js"></script>
<link href="libs/d3heatmapcore-0.0.0/heatmapcore.css" rel="stylesheet" />
<script src="libs/d3heatmapcore-0.0.0/heatmapcore.js"></script>
<script src="libs/d3-tip-0.6.6/index.js"></script>
<script src="libs/d3heatmap-binding-0.6.1.1/d3heatmap.js"></script>
<link href="libs/vis-4.20.1/vis.css" rel="stylesheet" />
<script src="libs/vis-4.20.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-2.0.2/visNetwork.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/book.css" type="text/css" />
<link rel="stylesheet" href="css/standard_html.css" type="text/css" />
<link rel="stylesheet" href="css/sem.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://m-clark.github.io/sem/"><span style="font-size:125%; font-variant:small-caps; font-style:italic; color:#990024; font-family:Roboto">Structural Equation Modeling</span></a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#statistical"><i class="fa fa-check"></i>Statistical</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#programming"><i class="fa fa-check"></i>Programming</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#outline"><i class="fa fa-check"></i>Outline</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#graphical-models"><i class="fa fa-check"></i>Graphical Models</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#latent-variables"><i class="fa fa-check"></i>Latent Variables</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#sem"><i class="fa fa-check"></i>SEM</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#others"><i class="fa fa-check"></i>Others</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#programming-language-choice"><i class="fa fa-check"></i>Programming Language Choice</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#setup"><i class="fa fa-check"></i>Setup</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i>Introduction to R</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-started"><i class="fa fa-check"></i>Getting Started</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installation"><i class="fa fa-check"></i>Installation</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#packages"><i class="fa fa-check"></i>Packages</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#rstudio"><i class="fa fa-check"></i>RStudio</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#importing-data"><i class="fa fa-check"></i>Importing Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#key-things-to-know-about-r"><i class="fa fa-check"></i>Key things to know about R</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-is-a-programming-language-not-a-stats-package"><i class="fa fa-check"></i>R is a programming language, not a ‘stats package’</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#never-ask-if-r-can-do-what-you-want.-it-can."><i class="fa fa-check"></i>Never ask if R can do what you want. It can.</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#main-components-script-console-graphics-device"><i class="fa fa-check"></i>Main components: script, console, graphics device</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-is-easy-to-use-but-difficult-to-master."><i class="fa fa-check"></i>R is easy to use, but difficult to master.</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#object-oriented"><i class="fa fa-check"></i>Object-oriented</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#case-sensitive"><i class="fa fa-check"></i>Case sensitive</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#the-lavaan-package"><i class="fa fa-check"></i>The lavaan package</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-help"><i class="fa fa-check"></i>Getting help</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#moving-forward"><i class="fa fa-check"></i>Moving forward</a><ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html"><i class="fa fa-check"></i>Graphical Models</a><ul>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#directed-graphs"><i class="fa fa-check"></i>Directed Graphs</a><ul>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#standard-linear-model"><i class="fa fa-check"></i>Standard linear model</a></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#path-analysis"><i class="fa fa-check"></i>Path Analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#bayesian-networks"><i class="fa fa-check"></i>Bayesian Networks</a></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#undirected-graphs"><i class="fa fa-check"></i>Undirected Graphs</a><ul>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#network-analysis"><i class="fa fa-check"></i>Network analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#summary-1"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="graphical-models-1.html"><a href="graphical-models-1.html#r-packages-used"><i class="fa fa-check"></i>R packages used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html"><i class="fa fa-check"></i>Latent Variables</a><ul>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#dimension-reductioncompression"><i class="fa fa-check"></i>Dimension Reduction/Compression</a><ul>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#principal-components-analysis"><i class="fa fa-check"></i>Principal Components Analysis</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#factor-analysis"><i class="fa fa-check"></i>Factor Analysis</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#other-techniques"><i class="fa fa-check"></i>Other Techniques</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#summary-2"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#constructs-and-measurement-models"><i class="fa fa-check"></i>Constructs and Measurement Models</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#other-issues-in-factor-analysis"><i class="fa fa-check"></i>Other issues in Factor Analysis</a><ul>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#some-specific-factor-models-in-sem"><i class="fa fa-check"></i>Some specific factor models in SEM</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#scale-development"><i class="fa fa-check"></i>Scale development</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#factor-scores"><i class="fa fa-check"></i>Factor Scores</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#terminology"><i class="fa fa-check"></i>Terminology</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#some-other-uses-of-latent-variables"><i class="fa fa-check"></i>Some Other Uses of Latent Variables</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#summary-3"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="latent-variables-1.html"><a href="latent-variables-1.html#r-packages-used-1"><i class="fa fa-check"></i>R packages used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html"><i class="fa fa-check"></i>Structural Equation Modeling</a><ul>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#measurement-model"><i class="fa fa-check"></i>Measurement Model</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#structural-model"><i class="fa fa-check"></i>Structural Model</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#the-process"><i class="fa fa-check"></i>The Process</a><ul>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#initial-considerations-of-complexity"><i class="fa fa-check"></i>Initial Considerations of Complexity</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#steps-to-take"><i class="fa fa-check"></i>Steps to Take</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#sem-example"><i class="fa fa-check"></i>SEM Example</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#issues-in-sem"><i class="fa fa-check"></i>Issues in SEM</a><ul>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#identification"><i class="fa fa-check"></i>Identification</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#fit"><i class="fa fa-check"></i>Fit</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#model-comparison"><i class="fa fa-check"></i>Model Comparison</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#observed-covariates"><i class="fa fa-check"></i>Observed covariates</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#interactions"><i class="fa fa-check"></i>Interactions</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#estimation"><i class="fa fa-check"></i>Estimation</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#missing-data"><i class="fa fa-check"></i>Missing data</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#other-sem-approaches"><i class="fa fa-check"></i>Other SEM approaches</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#how-to-fool-yourself-with-sem"><i class="fa fa-check"></i>How to fool yourself with SEM</a><ul>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#sample-size"><i class="fa fa-check"></i>Sample size</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#poor-data"><i class="fa fa-check"></i>Poor data</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#naming-a-latent-variable-doesnt-mean-it-exists"><i class="fa fa-check"></i>Naming a latent variable doesn’t mean it exists</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#ignoring-diagnostics"><i class="fa fa-check"></i>Ignoring diagnostics</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#ignoring-performance"><i class="fa fa-check"></i>Ignoring performance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#summary-4"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#terminology-1"><i class="fa fa-check"></i>Terminology</a></li>
<li class="chapter" data-level="" data-path="structural-equation-modeling.html"><a href="structural-equation-modeling.html#r-packages-used-2"><i class="fa fa-check"></i>R Packages Used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html"><i class="fa fa-check"></i>Latent Growth Curves</a><ul>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#random-effects"><i class="fa fa-check"></i>Random effects</a><ul>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#model-formality"><i class="fa fa-check"></i>Model formality</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#random-effects-in-sem"><i class="fa fa-check"></i>Random Effects in SEM</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#simulating-random-effects"><i class="fa fa-check"></i>Simulating Random Effects</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#running-a-growth-curve-model"><i class="fa fa-check"></i>Running a Growth Curve Model</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#thinking-more-generally-about-regression"><i class="fa fa-check"></i>Thinking more generally about regression</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#more-on-lgc"><i class="fa fa-check"></i>More on LGC</a><ul>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#lgc-are-non-standard-sem"><i class="fa fa-check"></i>LGC are non-standard SEM</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#residual-correlations"><i class="fa fa-check"></i>Residual correlations</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#nonlinear-time-effect"><i class="fa fa-check"></i>Nonlinear time effect</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#growth-mixture-models"><i class="fa fa-check"></i>Growth Mixture Models</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#other-covariates"><i class="fa fa-check"></i>Other covariates</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#some-differences-between-mixed-models-and-growth-curves"><i class="fa fa-check"></i>Some Differences between Mixed Models and Growth Curves</a><ul>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#random-slopes"><i class="fa fa-check"></i>Random slopes</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#wide-vs.long"><i class="fa fa-check"></i>Wide vs. long</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#sample-size-1"><i class="fa fa-check"></i>Sample size</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#number-of-time-points"><i class="fa fa-check"></i>Number of time points</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#balance"><i class="fa fa-check"></i>Balance</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#numbering-the-time-points"><i class="fa fa-check"></i>Numbering the time points</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#other-stuff"><i class="fa fa-check"></i>Other stuff</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#summary-5"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="latent-growth-curves.html"><a href="latent-growth-curves.html#r-packages-used-3"><i class="fa fa-check"></i>R Packages Used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html"><i class="fa fa-check"></i>Mixture Models</a><ul>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#a-motivating-example"><i class="fa fa-check"></i>A Motivating Example</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#create-clustered-data"><i class="fa fa-check"></i>Create Clustered Data</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#mixture-modeling-with-old-faithful"><i class="fa fa-check"></i>Mixture modeling with Old Faithful</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#sem-and-latent-categorical-variables"><i class="fa fa-check"></i>SEM and Latent Categorical Variables</a><ul>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#latent-categories-vs.multi-group-analysis"><i class="fa fa-check"></i>Latent Categories vs. Multi-group Analysis</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#latent-trajectories"><i class="fa fa-check"></i>Latent Trajectories</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#estimation-1"><i class="fa fa-check"></i>Estimation</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#terminology-in-sem"><i class="fa fa-check"></i>Terminology in SEM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#summary-6"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="mixture-models.html"><a href="mixture-models.html#r-packages-used-4"><i class="fa fa-check"></i>R Packages Used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html"><i class="fa fa-check"></i>Item Response Theory</a><ul>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#standard-models"><i class="fa fa-check"></i>Standard Models</a><ul>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#one-parameter-model"><i class="fa fa-check"></i>One Parameter Model</a></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#two-parameter-model"><i class="fa fa-check"></i>Two Parameter Model</a></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#three-parameter-model"><i class="fa fa-check"></i>Three Parameter Model</a></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#four-parameter-model"><i class="fa fa-check"></i>Four Parameter Model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#other-irt-models"><i class="fa fa-check"></i>Other IRT Models</a><ul>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#additional-covariates"><i class="fa fa-check"></i>Additional covariates</a></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#graded-response-model"><i class="fa fa-check"></i>Graded Response Model</a></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#multidimensional-irt"><i class="fa fa-check"></i>Multidimensional IRT</a></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#other-irt"><i class="fa fa-check"></i>Other IRT</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#summary-7"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#irt-terminology"><i class="fa fa-check"></i>IRT Terminology</a></li>
<li class="chapter" data-level="" data-path="item-response-theory.html"><a href="item-response-theory.html#r-packages-used-5"><i class="fa fa-check"></i>R Packages Used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="topic-models.html"><a href="topic-models.html"><i class="fa fa-check"></i>Topic Models</a><ul>
<li class="chapter" data-level="" data-path="topic-models.html"><a href="topic-models.html#latent-dirichlet-allocation"><i class="fa fa-check"></i>Latent Dirichlet Allocation</a></li>
<li class="chapter" data-level="" data-path="topic-models.html"><a href="topic-models.html#analysis"><i class="fa fa-check"></i>Analysis</a></li>
<li class="chapter" data-level="" data-path="topic-models.html"><a href="topic-models.html#summary-of-topic-models"><i class="fa fa-check"></i>Summary of Topic Models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bayesian-nonparametric-models.html"><a href="bayesian-nonparametric-models.html"><i class="fa fa-check"></i>Bayesian Nonparametric Models</a><ul>
<li class="chapter" data-level="" data-path="bayesian-nonparametric-models.html"><a href="bayesian-nonparametric-models.html#chinese-restaurant-process"><i class="fa fa-check"></i>Chinese Restaurant Process</a></li>
<li class="chapter" data-level="" data-path="bayesian-nonparametric-models.html"><a href="bayesian-nonparametric-models.html#indian-buffet-process"><i class="fa fa-check"></i>Indian Buffet Process</a></li>
<li class="chapter" data-level="" data-path="bayesian-nonparametric-models.html"><a href="bayesian-nonparametric-models.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="bayesian-nonparametric-models.html"><a href="bayesian-nonparametric-models.html#r-packages-used-6"><i class="fa fa-check"></i>R packages used</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i>Exercises</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#data-set-descriptions"><i class="fa fa-check"></i>Data Set Descriptions</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#mcclelland"><i class="fa fa-check"></i>McClelland</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#national-longitudinal-survey-of-youth-1997-nlsy97"><i class="fa fa-check"></i>National Longitudinal Survey of Youth (1997, NLSY97)</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#wheaton-1977-data"><i class="fa fa-check"></i>Wheaton 1977 data</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#harman-5"><i class="fa fa-check"></i>Harman 5</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#big-five"><i class="fa fa-check"></i>Big Five</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#old-faithful"><i class="fa fa-check"></i>Old Faithful</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#harman-1974"><i class="fa fa-check"></i>Harman 1974</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#marsh-hocevar-1985"><i class="fa fa-check"></i>Marsh &amp; Hocevar 1985</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#abortion-attitudes"><i class="fa fa-check"></i>Abortion Attitudes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#terminology-in-sem-1"><i class="fa fa-check"></i>Terminology in SEM</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#problematic-andor-not-very-useful-terms"><i class="fa fa-check"></i>Problematic and/or not very useful terms</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#lavaan-output-explained"><i class="fa fa-check"></i>Lavaan Output Explained</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#code-examples"><i class="fa fa-check"></i>Code Examples</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#factor-analysis-via-maximum-likelihood"><i class="fa fa-check"></i>Factor Analysis via Maximum Likelihood</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#parallel-process-example"><i class="fa fa-check"></i>Parallel Process Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#causal-bias"><i class="fa fa-check"></i>Causal Bias</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#prediction-1"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#chance"><i class="fa fa-check"></i>Chance</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#other"><i class="fa fa-check"></i>Other</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#some-references"><i class="fa fa-check"></i>Some references</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#software-revisited"><i class="fa fa-check"></i>Software Revisited</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#mplus"><i class="fa fa-check"></i>Mplus</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#r"><i class="fa fa-check"></i>R</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#stata"><i class="fa fa-check"></i>Stata</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#other-2"><i class="fa fa-check"></i>Other</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#resources"><i class="fa fa-check"></i>Resources</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#graphical-models-2"><i class="fa fa-check"></i>Graphical Models</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#potential-outcomes"><i class="fa fa-check"></i>Potential Outcomes</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#measurement-models-including-irt"><i class="fa fa-check"></i>Measurement Models (including IRT)</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#applied-sem"><i class="fa fa-check"></i>Applied SEM</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#nonparametric-models"><i class="fa fa-check"></i>Nonparametric models</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#lavaan"><i class="fa fa-check"></i>lavaan</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#other-sem-tools-in-r"><i class="fa fa-check"></i>Other SEM tools in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="other-3.html"><a href="other-3.html"><i class="fa fa-check"></i>Other</a></li>
<li><a href="section-1.html#section-1"></a></li>
<li class="divider"></li>
<li><a href="https://m-clark.github.io/"><img src="img/mc.png" style="width:50%; padding:0px 0; display:block; margin: 0 auto;" alt="MC logo"></a></li>
<li><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="width:50%; border-width:0; display:block; margin: 0 auto;" src="img/ccbysa_test.png" /></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><span style="font-size:150%; font-variant:small-caps; font-style:italic; ">Graphical and Latent Variable Modeling</span></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="latent-variables-1" class="section level1">
<h1>Latent Variables</h1>
<p>Not everything we want to measure comes with an obvious yardstick. If one wants to measure something like a person’s happiness, what would they have at their disposal?</p>
<ul>
<li>Are they smiling?</li>
<li>Did they just get a pay raise?</li>
<li>Are they interacting well with others?</li>
<li>Are they relatively healthy?</li>
</ul>
<p>Any of these might be useful as an <em>indicator</em> of their current state of happiness, but of course none of them would tell us whether they truly are happy or not. At best, they can be considered imperfect measures. If we consider those and other indicators collectively, perhaps we can get an underlying measure of something we might call happiness, contentment, or some other arbitrary but descriptive name.</p>
<p>Despite the above depiction of a latent variable, which is consistent with how they are typically used within psychology, education and related fields, the use of <span class="emph">latent variable</span> models is actually seen all over, and in ways that may have little to do with what we will be mostly focusing on here<a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a>. Broadly speaking, <span class="emph">factor analysis</span> can be seen as a dimension reduction technique, or as an approach to modeling measurement error and understanding underlying constructs. We will give some description of the former while focusing on the latter.</p>
<div id="dimension-reductioncompression" class="section level2">
<h2>Dimension Reduction/Compression</h2>
<p>Before getting into what we’ll call <span class="emph">measurement models</span> in the SEM context, we can first take a look at things from a more general perspective, especially in terms of dimension reduction. Many times we simply have the goal of taking a whole lot of variables, reducing them to much fewer, but while retaining as much information about the originals as possible. For example, this is an extremely common goal in areas of image and audio compression. Statistical techniques amenable to these approaches are commonly referred to as <span class="emph">matrix factorization</span>.</p>
<div id="principal-components-analysis" class="section level3">
<h3>Principal Components Analysis</h3>
<p>Probably the most commonly used factor-analytic technique is <span class="emph">principal components analysis</span> (PCA). It seeks to extract <em>components</em> from a set of variables, with each component containing as much of the original variance as possible. Equivalently, it can be seen as producing projections on a lower dimensional subspace that have a minimum distance from the original data. Components are linear combination of the original variables.</p>
<p>PCA works on a covariance/correlation matrix, and it will return as many components as there are variables that go into the analysis. Each subsequent component accounts for less variance than the previous component, and summing all components results in 100% of the total variance in the original data accounted for. With appropriate steps, the components can completely reproduce the original data/correlation matrix. However, as the goal is dimension reduction, we only want to retain some of these components, and so the reproduced matrix will not be exact. This however gives us some sense of a goal to shoot for, and the same idea is also employed in factor analysis/SEM, where we also work with the covariance matrix and prefer models that can closely reproduce the original correlations seen with the observed data.</p>
<p>Visually, we can display PCA as a graphical model. Here is one with four components/variables. The size of the components represents the amount of variance each accounts for.</p>
<div style="width:50%; margin:0 auto; font-family:Roboto; font-size:50%">
<div id="htmlwidget-ad54536a7f87ecfaa6f1" style="width:100%;height:400px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-ad54536a7f87ecfaa6f1">{"x":{"diagram":"  digraph PCA {\n    # Intialization of graph attributes\n    graph [overlap = false  bgcolor=transparent]\n  \n    # Initialization of node attributes\n    node [shape = circle fontname = Roboto color = gray80 type = box fixedsize = true penwidth=2 fontcolor=gray75]\n  \n  \n    # Node statements\n    PC3 [label=<PC<sub>3<\/sub>> width=.75];\n    PC4 [label=<PC<sub>4<\/sub>> width=.5]; \n    PC1 [label=<PC<sub>1<\/sub>> width=1.5, style=filled fillcolor=\"#b2001d80\", fontcolor=\"white\"]; \n    PC2 [label=<PC<sub>2<\/sub>> width=1.25];    # reordered to deal with unusual ordering otherwise\n\n    node [width=1 shape=square color=gray25]\n    X3 [label=<X<sub>3<\/sub>>]; \n    X4 [label=<X<sub>4<\/sub>>]; \n    X1 [label=<X<sub>1<\/sub>>]; \n    X2 [label=<X<sub>2<\/sub>>]; \n  \n    # Xnitialization of edge attributes\n    edge [color = gray50 rel = yields]\n  \n    # Edge statements\n    edge [arrowhead=none penwidth=2]\n    PC1 -> {X1 X2 X3 X4}[color=\"#b2001d50\"];\n    {PC2 PC3 PC4} -> {X1 X2 X3 X4};\n\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<p><br></p>
<p>Let’s see an example. The following regards a very small data set<a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a> 5 socioeconomic indicators for 12 census tracts in Los Angeles (a classic example from <a href="appendix.html#harman-5">Harman, 1967</a>). We’ll use the <span class="pack">psych</span> package, and the <span class="func">principal</span> function within it. To use the function we provide the data (available via the package), specify the number of components/factors we want to retain, and other options (in this case, a <em>rotated</em> solution might be a little more interpretable<a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a>, but is typically not employed in PCA, so we specify ‘none’). The <span class="pack">psych</span> package gives us more options and a little more output than standard PCA packages and functions, and one that is more consistent with the factor analysis technique we’ll spend time with later. While we will also use <span class="pack">lavaan</span> for factor analysis to be consistent with the SEM approach, the <span class="pack">psych</span> package is a great tool for standard factor analysis, assessing scale reliability, and other fun stuff.</p>
<p>For the PCA, we’ll retain three components and use no rotation, and we’ll also focus on a standardized solution. Not doing so would result in components favoring variables with more variance relative to others. As such standardization is almost always conducted as a pre-processing step for PCA, though here it is an option to specify as part of the function (<code>covar=F</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(psych)
pc =<span class="st"> </span><span class="kw">principal</span>(Harman.<span class="dv">5</span>, <span class="dt">nfactors=</span><span class="dv">2</span>,  <span class="dt">rotate=</span><span class="st">&#39;none&#39;</span>, <span class="dt">covar =</span> F)
pc</code></pre></div>
<pre><code>Principal Components Analysis
Call: principal(r = Harman.5, nfactors = 2, rotate = &quot;none&quot;, covar = F)
Standardized loadings (pattern matrix) based upon correlation matrix
              PC1   PC2   h2    u2 com
population   0.58  0.81 0.99 0.012 1.8
schooling    0.77 -0.54 0.89 0.115 1.8
employment   0.67  0.73 0.98 0.021 2.0
professional 0.93 -0.10 0.88 0.120 1.0
housevalue   0.79 -0.56 0.94 0.062 1.8

                       PC1  PC2
SS loadings           2.87 1.80
Proportion Var        0.57 0.36
Cumulative Var        0.57 0.93
Proportion Explained  0.62 0.38
Cumulative Proportion 0.62 1.00

Mean item complexity =  1.7
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.03 
 with the empirical chi square  0.29  with prob &lt;  0.59 

Fit based upon off diagonal values = 1</code></pre>
<p>First focus on the portion of the output where it says <code>SS loadings</code> . The first line is the sum of the squared loadings<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a> for each component (in this case where we are using a correlation matrix, summing across all 5 possible components would equal the value of 5). The <code>Proportion Var</code> tells us how much of the overall variance the component accounts for out of all the variables (e.g. 2.87 / 5 = 0.57). The <code>Cumulative Var</code> tells us that 2 components make up over 93% the variance. The others are the same thing just based on the 2 retained components rather than all 5 variables (i.e. the cumulative explained variance would = 1). We can see that the second component accounts for less variance, and this would continue with additional components, where each accounts for a decreasing amount of variance.</p>
<p><span class="emph">Loadings</span>, also referred to as the <span class="emph">pattern matrix</span>, in this scenario represent the estimated correlation of an item with its component, and provide the key way in which we interpret the factors. As an example, we can reproduce the loadings by correlating the observed variables with the estimated component scores, something we’ll talk more about later.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(Harman.<span class="dv">5</span>, pc<span class="op">$</span>scores) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">2</span>)</code></pre></div>
<pre><code>              PC1   PC2
population   0.58  0.81
schooling    0.77 -0.54
employment   0.67  0.73
professional 0.93 -0.10
housevalue   0.79 -0.56</code></pre>
<p>It can be difficult to sort it out just by looking at the values, so we’ll look at it visually. In the following plot, stronger loadings are indicated by blue, and we can see the different variables associated with different components.</p>
<div style="width:50%; margin-left:auto;  margin-right:auto" width="500">
<div id="12a243b2e497" style="width:100%;height:400px;" class="plotly html-widget"></div>
<script type="application/json" data-for="12a243b2e497">{"x":{"data":[{"x":[1,2],"y":[1,2,3,4,5],"z":[[0.905250214054062,0],[1,0.304496566169873],[0.825596349862747,0.861565035788644],[0.889065894131514,0.009005472719384],[0.764227783002789,0.915487844482278]],"text":[["value:  0.7911612<br />column: PC1<br />row: housevalue","value: -0.5581795<br />column: PC2<br />row: housevalue"],["value:  0.9323926<br />column: PC1<br />row: professional","value: -0.1043054<br />column: PC2<br />row: professional"],["value:  0.6724314<br />column: PC1<br />row: employment","value:  0.7260453<br />column: PC2<br />row: employment"],["value:  0.7670373<br />column: PC1<br />row: schooling","value: -0.5447561<br />column: PC2<br />row: schooling"],["value:  0.5809571<br />column: PC1<br />row: population","value:  0.8064212<br />column: PC2<br />row: population"]],"colorscale":[[0,"#D86450"],[0.009005472719384,"#D96551"],[0.304496566169873,"#81A9CA"],[0.764227783002789,"#2267AD"],[0.825596349862747,"#1F61A5"],[0.861565035788644,"#1D5C9F"],[0.889065894131514,"#1B599A"],[0.905250214054062,"#195594"],[0.915487844482278,"#175290"],[1,"#0C3D73"]],"type":"heatmap","showscale":false,"autocolorscale":false,"showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[1],"name":"99_073810938e4bd48c4033c1577831c181","type":"scatter","mode":"markers","opacity":0,"hoverinfo":"none","showlegend":false,"marker":{"color":[0,1],"colorscale":[[0,"#DB6A55"],[0.0714285714285715,"#E0755C"],[0.142857142857143,"#F4B4A0"],[0.214285714285714,"#E2D8D5"],[0.285714285714286,"#7CA7C9"],[0.357142857142857,"#418CC0"],[0.428571428571429,"#367FB8"],[0.5,"#3177B5"],[0.571428571428572,"#2B70B2"],[0.642857142857143,"#2569AE"],[0.714285714285714,"#2267AD"],[0.785714285714286,"#2267AD"],[0.857142857142857,"#1E5FA2"],[0.928571428571429,"#185392"],[1,"#0F427A"]],"colorbar":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"thickness":23.04,"title":null,"titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"tickmode":"array","ticktext":["-0.5","0.0","0.5"],"tickvals":[7.93016446160826e-17,0.357142857142857,0.714285714285714],"tickfont":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"ticklen":2,"len":0.5},"showscale":false},"xaxis":"x","yaxis":"y","frame":null}],"layout":{"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[0.5,2.5],"ticktext":["PC1","PC2"],"tickvals":[1,2],"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":13.2835201328352},"tickangle":-45,"showline":false,"linecolor":"rgba(0,0,0,1)","linewidth":0.66417600664176,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"y","title":"","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[0.5,5.5],"ticktext":["housevalue","professional","employment","schooling","population"],"tickvals":[1,2,3,4,5],"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":13.2835201328352},"tickangle":-0,"showline":false,"linecolor":"rgba(0,0,0,1)","linewidth":0.66417600664176,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"x","title":"","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"margin":{"t":0,"r":null,"b":23.9103362391034,"l":95.6413449564135},"paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"showlegend":true,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":1,"yanchor":"top"},"hovermode":"closest","title":""},"attrs":{"12a24533621ef":{"fill":{},"x":{},"y":{},"type":"ggplotly"}},"source":"A","config":{"modeBarButtonsToAdd":[],"cloud":false,"displaylogo":false,"modeBarButtonsToRemove":["sendDataToCloud","select2d","lasso2d","autoScale2d","hoverClosestCartesian","hoverCompareCartesian","sendDataToCloud"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"subplot":true,"base_url":"https://plot.ly"},"evals":[],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
</div>
<p><br></p>
<p>Interpretation is the fun but commonly difficult part. As an example, PC1 looks to be mostly our socioeconomic status of the tract. Any tract with a high score on that component will have high values on all variables. It’s worth mentioning the <span class="emph">naming fallacy</span> at this point. Just because we associate a factor with some concept, doesn’t make it so. The underlying cause of the result could for example merely be due to population itself.</p>
<p>Some explanation of the other parts of the output:</p>
<ul>
<li><code>h2</code>: the amount of variance in the item/variable explained by the (retained) components. It is the sum of the squared loadings, a.k.a. <span class="emph">communality</span>. For example, population is almost completely explained by the three components</li>
<li><code>u2</code>: 1 - h2</li>
<li><code>com</code>: A measure of complexity. A value of 1 might be seen for something that loaded on only one component, and zero otherwise (a.k.a. perfect simple structure).</li>
</ul>
<p>We can get a quick graphical model displayed as follows<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a>:</p>
<div style="width:50%; margin-left:auto;  margin-right:auto; font-size:50%">
<div id="htmlwidget-1aaf3a08c403efa95c05" style="width:100%;height:400px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1aaf3a08c403efa95c05">{"x":{"diagram":"digraph Factor  {\n  graph [rankdir=RL  bgcolor=transparent]\n  \n  node [fontname=\"Roboto\" fontsize=10 fixedsize=TRUE, shape=box, width=1, color=\"#b2001d75\", fontcolor=\"gray50\"];\n  edge [fontname=\"Roboto\" fontsize=8 color=gray75 arrowhead=none fontcolor=\"gray50\"];\n  V1  [label = \"population\"];\n  V2  [label = \"schooling\"];\n  V3  [label = \"employment\"];\n  V4  [label = \"professional\"];\n  V5  [label = \"housevalue\"];\n  \n  node [shape=ellipse width =1.5 height=1.5 fillcolor=\"#b2001d80\" style=filled fontcolor=\"white\"];\n  PC1 [label=<PC<sub>1<\/sub>> width =1 height=1]; \n  PC2 [label=<PC<sub>2<\/sub>> width =.5 height=.5];\n  \n  \n  edge [color=\"#8B000094\"];\n  PC1-> V1 [ label = 0.58 penwidth=1.15];  # penwidths based on rescale of loadings to c(.5,2)\n  edge [color=\"#8B0000C4\"];\n  PC1-> V2 [ label = 0.77 penwidth=1.61];\n  edge [color=\"#8B0000AB\"];\n  PC1-> V3 [ label = 0.67 penwidth=1.37];\n  edge [color=\"#8B0000EE\"];\n  PC1-> V4 [ label = 0.93 penwidth=2.0];\n  edge [color=\"#8B0000CA\"];\n  PC1-> V5 [ label = 0.79 penwidth=1.66];\n  \n  \n  edge [color=\"#8B0000CE\"];\n  PC2-> V1 [ label = 0.81 penwidth=1.71];\n  edge [color=\"#228B22CE\"];\n  PC2-> V2 [ label = -0.54 penwidth=1.06];\n  edge [color=\"#8B0000B9\"];\n  PC2-> V3 [ label = 0.73 penwidth=1.52];\n  edge [color=\"#228B228E\"];\n  PC2-> V5 [ label = -0.56 penwidth=1.10];\n  \n  \n  { rank=same;\n  V1;V2;V3;V4;V5;}\n  { rank=same;\n  PC1;PC2;}\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<p><br></p>
<p>PCA may not be the best choice in this scenario, nor likely is this the most interpretable solution. One issue with PCA is that this graphical model assumes the observed variables are measured without error. In addition, the principal components do not correlate with one another by default, but it seems likely that we would want to allow the latent variables to do so in many situations, perhaps including this one (a different rotation would allow this). However, if our goal is merely to reduce the 24 items to a few that account for the most variance, this would be a standard technique.</p>
</div>
<div id="factor-analysis" class="section level3">
<h3>Factor Analysis</h3>
<p>Now let’s examine what is sometimes called <em>common</em> factor analysis, and also sometimes <em>exploratory</em> factor analysis in the social sciences, and even ‘classical’ or ‘traditional’, but typically just factor analysis (FA) everywhere else. While we can use both PCA and FA for similar reasons (dimension reduction) and even have similar interpretations (in terms of loadings), there are some underlying subtleties between the two that provide unique distinctions<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a>. Noting these distinctions with some detail will require some matrix notation, but for readers not so keen on such presentation they may note the images and concluding points.</p>
<p>First let’s revisit PCA, where we can depict it conceptually as an approach where we attempt to approximate the correlation matrix in terms of the product of components, represented by our loading matrix <span class="math inline">\(L\)</span><a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a>.</p>
<p><span class="math display">\[R = LL&#39;\]</span> and</p>
<p><span class="math display">\[C = XW\]</span> In other words, each component <em>score</em> <span class="math inline">\(C\)</span>, i.e. the score for a particular observation with regard to the component, is a weighted combination of the <span class="math inline">\(p\)</span> observed variables <span class="math inline">\(X\)</span>, with weight matrix <span class="math inline">\(W\)</span><a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a>. We can use those component loadings to approximate the correlation matrix, or reproduce it exactly if we retain all the components possible. The following demonstrates this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pc_all =<span class="st"> </span><span class="kw">principal</span>(Harman.<span class="dv">5</span>, <span class="dt">nfactors=</span><span class="dv">5</span>,  <span class="dt">rotate=</span><span class="st">&#39;none&#39;</span>, <span class="dt">covar =</span> F)
<span class="kw">all.equal</span>(<span class="kw">tcrossprod</span>(<span class="kw">loadings</span>(pc_all)), <span class="kw">cor</span>(Harman.<span class="dv">5</span>))</code></pre></div>
<pre><code>[1] TRUE</code></pre>
<div style="width:50%; margin-left:auto;  margin-right:auto;">
<div id="htmlwidget-aee41a5a9e808e8c49cf" style="width:100%;height:50%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-aee41a5a9e808e8c49cf">{"x":{"diagram":"digraph composite {\n  # Intialization of graph attributes\n  graph [overlap = false fill=none bgcolor=transparent]\n\n  # Initialization of node attributes\n  node [shape = rectangle style=filled fillcolor=\"#1f65b780\" color=gray80 width=.25 height=.25 fontcolor=white fontname=Roboto fixedsize=true]\n\n  # Node statements\n  Component [width=1.5 height=1 shape = circle color=\"#1f65b7\"];\n\n  node [width=1 shape=square color=\"#1f65b780\" fillcolor=none fontcolor=\"#1f65b775\" fontname=Roboto fixedsize=true]\n    X1 [label=<X<sub>1<\/sub>>]; \n    X2 [label=<X<sub>2<\/sub>>]; \n    X3 [label=<X<sub>3<\/sub>>]; \n    X4 [label=<X<sub>4<\/sub>>]; \n\n  # Xnitialization of edge attributes\n  edge [color = gray50 rel = yields]\n\n  # Edge statements\n  {X1 X2 X3 X4} -> Component;\n\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<p>If we return to our causal thinking from before, the causal flow originates with the observed variables to the components. Perhaps now it is clearer as to the interpretation of the loadings we had before, as correlations with components. In fact, we see now that the loadings are used to create the estimated component scores in the first place.</p>
<p>Things are different with factor analysis. Now the causal flow is in the other direction, originating with the latent variables.</p>
<p><span class="math display">\[X \approx FW\]</span></p>
<p>Each observed variable <span class="math inline">\(x\)</span> is a function of the latent variables that it is associated with. In addition, we also take into account the uniquenesses <span class="math inline">\(\Psi\)</span>, or that part which the factors do not explain.</p>
<div style="width:50%; margin:0 auto; text-align=&#39;center&#39;">
<div id="htmlwidget-a91fbb4e4d0c4d670ab7" style="width:100%;height:50%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-a91fbb4e4d0c4d670ab7">{"x":{"diagram":"digraph fa_gm {\n # Intialization of graph attributes\n graph [overlap = false rankdir=TB bgcolor=transparent]\n \n # Initialization of node attributes\n node [shape = circle fontname = Helvetica color = gray80 type = box fixedsize = true]\n \n # Node statements\n Factor  [width=1.5 height=1 shape = circle color=\"#1f65b7\" style=filled fillcolor=\"#1f65b780\" fontcolor=white];\n \n node [width=1 shape=square color=\"#1f65b780\" fontcolor=\"#1f65b775\" fontname=Roboto fixedsize=true]\n subgraph {\n    rank=same;\n    X1 [label=<X<sub>1<\/sub>>]; \n    X2 [label=<X<sub>2<\/sub>>]; \n    X3 [label=<X<sub>3<\/sub>>]; \n    X4 [label=<X<sub>4<\/sub>>]; \n }\n # Initialization of edge attributes\n edge [color = gray50 rel = yields]\n \n # Edge statements\n Factor -> X1 [style=dashed] ;\n Factor -> {X2 X3 X4};\n\n subgraph {\n    rank=same;\n    node [width=.5 height=.5 shape=circle fontcolor=white color=gray75 style=filled fillcolor=gray90 ]\n    U1 [label=<U<sub>1<\/sub>>]; \n    U2 [label=<U<sub>2<\/sub>>]; \n    U3 [label=<U<sub>3<\/sub>>]; \n    U4 [label=<U<sub>4<\/sub>>]; \n }\n\n  edge [dir=back]  \n  X1 -> U1;\n  X2 -> U2;\n  X3 -> U3;\n  X4 -> U4;\n\n\n \n }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<p>And to reproduce the correlation matrix:</p>
<p><span class="math display">\[R \approx LL&#39;\]</span> <span class="math display">\[R = LL&#39; + \Psi\]</span> So if we just use the loadings from the FA, we cannot reproduce the correlation matrix exactly, we need to add the uniquenesses as well<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a>. If you look at the <span class="objclass">pc_all</span> results, you’ll note that the uniquenesses are all zero, but this is not the case with factor analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fa_all =<span class="st"> </span><span class="kw">fa</span>(Harman.<span class="dv">5</span>, <span class="dt">nfactors=</span><span class="dv">5</span>,  <span class="dt">rotate=</span><span class="st">&#39;none&#39;</span>, <span class="dt">covar =</span> F, <span class="dt">fm =</span> <span class="st">&#39;gls&#39;</span>)
<span class="kw">all.equal</span>(<span class="kw">tcrossprod</span>(<span class="kw">loadings</span>(fa_all)), <span class="kw">cor</span>(Harman.<span class="dv">5</span>))           <span class="co"># doesn&#39;t reproduce cormat</span></code></pre></div>
<pre><code>[1] &quot;Mean relative difference: 0.0198653&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># add uniquenesses</span>
<span class="kw">all.equal</span>(<span class="kw">tcrossprod</span>(<span class="kw">loadings</span>(fa_all)) <span class="op">+</span><span class="st"> </span><span class="kw">diag</span>(fa_all<span class="op">$</span>uniquenesses), <span class="kw">cor</span>(Harman.<span class="dv">5</span>))  </code></pre></div>
<pre><code>[1] TRUE</code></pre>
<p>What this amounts to conceptually are a few key ideas:</p>
<ul>
<li>Factor analysis focuses on covariance. PCA focuses on variance.</li>
<li>Factors are the cause of the observed variables, variables are the cause of components.</li>
<li>Factor analysis does not assume perfect measurement of observed variables.</li>
</ul>
<p>Let’s now do the factor analysis for the same Harman 5 data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fac =<span class="st"> </span><span class="kw">fa</span>(Harman.<span class="dv">5</span>, <span class="dt">nfactors=</span><span class="dv">2</span>,  <span class="dt">rotate=</span><span class="st">&#39;none&#39;</span>)
fac</code></pre></div>
<pre><code>Factor Analysis using method =  minres
Call: fa(r = Harman.5, nfactors = 2, rotate = &quot;none&quot;)
Standardized loadings (pattern matrix) based upon correlation matrix
              MR1   MR2   h2     u2 com
population   0.62  0.78 1.00 0.0015 1.9
schooling    0.70 -0.52 0.77 0.2347 1.8
employment   0.70  0.68 0.96 0.0407 2.0
professional 0.88 -0.14 0.80 0.2029 1.1
housevalue   0.78 -0.60 0.97 0.0250 1.9

                       MR1  MR2
SS loadings           2.76 1.74
Proportion Var        0.55 0.35
Cumulative Var        0.55 0.90
Proportion Explained  0.61 0.39
Cumulative Proportion 0.61 1.00

Mean item complexity =  1.7
Test of the hypothesis that 2 factors are sufficient.

The degrees of freedom for the null model are  10  and the objective function was  6.38 with Chi Square of  54.25
The degrees of freedom for the model are 1  and the objective function was  0.35 

The root mean square of the residuals (RMSR) is  0.01 
The df corrected root mean square of the residuals is  0.03 

The harmonic number of observations is  12 with the empirical chi square  0.02  with prob &lt;  0.88 
The total number of observations was  12  with Likelihood Chi Square =  2.5  with prob &lt;  0.11 

Tucker Lewis Index of factoring reliability =  0.582
RMSEA index =  0.507  and the 90 % confidence intervals are  0 0.972
BIC =  0.01
Fit based upon off diagonal values = 1</code></pre>
<p>Once again, we’ll depict the loadings visually.</p>
<div style="width:50%; margin-left:auto;  margin-right:auto" width="500">
<div id="12a243de35b69" style="width:100%;height:400px;" class="plotly html-widget"></div>
<script type="application/json" data-for="12a243de35b69">{"x":{"data":[{"x":[1,2],"y":[1,2,3,4,5],"z":[[0.932408396917369,0],[1,0.309875796758276],[0.879254422547952,0.866931193746985],[0.879381723570235,0.0554848353624195],[0.824857382169175,0.933973237544094]],"text":[["value:  0.7806362<br />column: MR1<br />row: housevalue","value: -0.6046267<br />column: MR2<br />row: housevalue"],["value:  0.8810559<br />column: MR1<br />row: professional","value: -0.1442497<br />column: MR2<br />row: professional"],["value:  0.7016662<br />column: MR1<br />row: employment","value:  0.6833578<br />column: MR2<br />row: employment"],["value:  0.7018554<br />column: MR1<br />row: schooling","value: -0.5221939<br />column: MR2<br />row: schooling"],["value:  0.6208495<br />column: MR1<br />row: population","value:  0.7829610<br />column: MR2<br />row: population"]],"colorscale":[[0,"#CF5446"],[0.0554848353624195,"#D86551"],[0.309875796758276,"#719CC4"],[0.824857382169175,"#2267AD"],[0.866931193746985,"#2267AD"],[0.879254422547952,"#2267AD"],[0.879381723570235,"#2267AD"],[0.932408396917369,"#1C5C9E"],[0.933973237544094,"#1C5B9D"],[1,"#134B87"]],"type":"heatmap","showscale":false,"autocolorscale":false,"showlegend":false,"xaxis":"x","yaxis":"y","hoverinfo":"text","frame":null},{"x":[1],"y":[1],"name":"99_3a103d521d2b5ea7e064486c2abebf66","type":"scatter","mode":"markers","opacity":0,"hoverinfo":"none","showlegend":false,"marker":{"color":[0,1],"colorscale":[[0,"#CF5547"],[0.0714285714285715,"#DA6A54"],[0.142857142857143,"#EA9179"],[0.214285714285714,"#FAD4C4"],[0.285714285714286,"#B0BBCD"],[0.357142857142857,"#3D89BE"],[0.428571428571429,"#3882BA"],[0.5,"#337BB7"],[0.571428571428572,"#2E73B4"],[0.642857142857143,"#2D72B3"],[0.714285714285714,"#2C71B2"],[0.785714285714286,"#266AAF"],[0.857142857142857,"#2267AD"],[0.928571428571429,"#2267AD"],[1,"#1A5999"]],"colorbar":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"thickness":23.04,"title":null,"titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"tickmode":"array","ticktext":["-0.5","0.0","0.5"],"tickvals":[0.0714285714285715,0.428571428571429,0.785714285714286],"tickfont":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"ticklen":2,"len":0.5},"showscale":false},"xaxis":"x","yaxis":"y","frame":null}],"layout":{"xaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[0.5,2.5],"ticktext":["MR1","MR2"],"tickvals":[1,2],"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":13.2835201328352},"tickangle":-45,"showline":false,"linecolor":"rgba(0,0,0,1)","linewidth":0.66417600664176,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"y","title":"","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"yaxis":{"domain":[0,1],"type":"linear","autorange":false,"tickmode":"array","range":[0.5,5.5],"ticktext":["housevalue","professional","employment","schooling","population"],"tickvals":[1,2,3,4,5],"ticks":"outside","tickcolor":"rgba(51,51,51,1)","ticklen":3.65296803652968,"tickwidth":0.66417600664176,"showticklabels":true,"tickfont":{"color":"rgba(77,77,77,1)","family":"","size":13.2835201328352},"tickangle":-0,"showline":false,"linecolor":"rgba(0,0,0,1)","linewidth":0.66417600664176,"showgrid":false,"gridcolor":null,"gridwidth":0,"zeroline":false,"anchor":"x","title":"","titlefont":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"hoverformat":".2f"},"shapes":[{"type":"rect","fillcolor":null,"line":{"color":null,"width":0,"linetype":[]},"yref":"paper","xref":"paper","x0":0,"x1":1,"y0":0,"y1":1}],"margin":{"t":0,"r":null,"b":23.9103362391034,"l":95.6413449564135},"paper_bgcolor":"rgba(255,255,255,1)","font":{"color":"rgba(0,0,0,1)","family":"","size":14.6118721461187},"showlegend":true,"legend":{"bgcolor":"rgba(255,255,255,1)","bordercolor":"transparent","borderwidth":1.88976377952756,"font":{"color":"rgba(0,0,0,1)","family":"","size":11.689497716895},"y":1,"yanchor":"top"},"hovermode":"closest","title":""},"attrs":{"12a24d922b44":{"fill":{},"x":{},"y":{},"type":"ggplotly"}},"source":"A","config":{"modeBarButtonsToAdd":[],"cloud":false,"displaylogo":false,"modeBarButtonsToRemove":["sendDataToCloud","select2d","lasso2d","autoScale2d","hoverClosestCartesian","hoverCompareCartesian","sendDataToCloud"]},"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1}},"subplot":true,"base_url":"https://plot.ly"},"evals":[],"jsHooks":{"render":[{"code":"function(el, x) { var ctConfig = crosstalk.var('plotlyCrosstalkOpts').set({\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1}}); }","data":null}]}}</script>
</div>
<p><br></p>
<p>We have a notably different interpretation with this approach. To begin, our first two factors account for roughly the same amount of variance. Secondly, none reflect all the variables simultaneously. For example, the first might reflect the wealth of census tracts, while the second size. The third latent variable has a murky interpretation at best, but is mostly reflective of education. However, for identifiability reasons, a topic we will address more later, technically 3 factors is too many for five variables without imposing constraints on some of the parameters.</p>
<div id="correlated-constructs" class="section level4">
<h4>Correlated constructs</h4>
<p>Another thing to keep in mind with factor analysis is that the default setting<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a> allows correlated latent variables, unlike with PCA, which instead produces orthogonal components by default. This leads to an issue interpreting the loadings as ‘mere correlations’ of observed variables to the latent variable. If we had only one factor, or forced the factors to have zero correlation as is the case with PCA, we could still interpret the loadings as such. But now, even if an observed variable has a loading of 0 for Factor 1, it still may be correlated with that factor because it has a strong loading for Factor 2 and Factor 2 is correlated with Factor 1. As such, with factor analysis (or PCA with non-orthogonal rotations) we distinguish the following definitions that will carry over to the SEM setting:</p>
<ul>
<li><span class="emph">pattern coefficients</span>: these are the standard loadings you see</li>
<li><span class="emph">structure coefficients</span>: the correlation of an observed variable with a factor that reflects <em>any</em> association, causal or otherwise.</li>
</ul>
<p>As noted these are identical if there is only one factor or the factors are orthogonal. To demonstrate the difference, let’s rerun the analysis with 2 factors and allow them to correlate. By default the <span class="func">fa</span> function uses a oblimin rotation that will do this. Then we can examine the structure coefficients versus the loadings. Note that when you run this you will get a warning, which amounts to telling you 12 observations isn’t enough. I also add a little bit, <code>[]</code>, to get rid of the psych package’s unfortunate default printing of these matrices, which includes information beyond the matrices themselves.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fac2 =<span class="st"> </span><span class="kw">fa</span>(Harman.<span class="dv">5</span>, <span class="dt">nfactors=</span><span class="dv">2</span>)
fac2<span class="op">$</span>loadings[] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">2</span>)</code></pre></div>
<pre><code>               MR1   MR2
population   -0.07  1.01
schooling     0.88 -0.07
employment    0.06  0.97
professional  0.76  0.36
housevalue    1.00 -0.09</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fac2<span class="op">$</span>Structure[] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">2</span>)</code></pre></div>
<pre><code>              MR1  MR2
population   0.11 1.00
schooling    0.87 0.09
employment   0.23 0.98
professional 0.82 0.49
housevalue   0.98 0.08</code></pre>
</div>
<div id="the-number-of-factors" class="section level4">
<h4>The number of factors</h4>
<p>A key question, even in the context of SEM, is the number of latent variables to posit for a particular set of variables. In SEM this is usually guided by theory, but in common factor analysis, all items load on whatever number of factors we choose. In <em>both</em> cases some solutions will work better than others for the data at hand, and different models should be compared. The same techniques used in other modeling situations, e.g. comparing AIC, are often available. Furthermore, one should inspect the residuals of the the observed vs. reproduced correlation matrix to further understand where a particular model fails, even if it seems to be a decent fit otherwise.</p>
<p>As an example, comparing the BIC of a two factor model vs. a one factor model would show that the two factor model is preferable (lower BIC), and aside from a couple of the variances, the residuals are quite low.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fac1 =<span class="st"> </span><span class="kw">fa</span>(Harman.<span class="dv">5</span>, <span class="dt">nfactors=</span><span class="dv">1</span>,  <span class="dt">rotate=</span><span class="st">&#39;none&#39;</span>, <span class="dt">fm=</span><span class="st">&#39;ml&#39;</span>)
fac2 =<span class="st"> </span><span class="kw">fa</span>(Harman.<span class="dv">5</span>, <span class="dt">nfactors=</span><span class="dv">2</span>,  <span class="dt">rotate=</span><span class="st">&#39;none&#39;</span>, <span class="dt">fm=</span><span class="st">&#39;ml&#39;</span>)

fac1<span class="op">$</span>BIC</code></pre></div>
<pre><code>[1] 12.13712</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fac2<span class="op">$</span>BIC</code></pre></div>
<pre><code>[1] -0.2618109</code></pre>
<div style="width:50%; margin:0 auto">
<div id="htmlwidget-9d8eb07eb1add9f84382" style="width:100%;height:500px;" class="d3heatmap html-widget"></div>
<script type="application/json" data-for="htmlwidget-9d8eb07eb1add9f84382">{"x":{"rows":null,"cols":null,"matrix":{"data":["0.005","-0.005","0","0.001","0.001","-0.005","0.193","0.036","-0.023","-0.001","0","0.036","0.036","-0.005","-0.011","0.001","-0.023","-0.005","0.185","0.01","0.001","-0.001","-0.011","0.01","0.074"],"dim":[5,5],"rows":["population","schooling","employment","professional","housevalue"],"cols":["population","schooling","employment","professional","housevalue"]},"image":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAWUlEQVQImTXISw5AMBRA0UsanxiUkQXY/2ZYhvhVUtXKewbiDE/mo2odd0LZ8svruDOuhjsJlZ+p4kF2JdU7CQBWHKiQA9i0YMUxbQZfdl9iClBh6BvO8PACBAUoWTwmlbMAAAAASUVORK5CYII=","theme":null,"options":{"xaxis_height":80,"yaxis_width":120,"xaxis_font_size":10,"yaxis_font_size":10,"brush_color":"#0000FF","show_grid":true,"anim_duration":500,"yclust_width":0,"xclust_height":0}},"evals":[],"jsHooks":[]}</script>
</div>
<p><br></p>
<p>The main thing you need to know is that every time you use a scree plot to determine the number of factors to retain, both a kitten <em>and</em> a puppy will die in some gruesome fashion.</p>
</div>
<div id="estimation-in-factor-analysis" class="section level4">
<h4>Estimation in Factor Analysis</h4>
<p>There are several methods available to estimate a factor analysis.</p>
<ul>
<li><strong>Principal Axis</strong>: essentially PCA with communalities on the diagonal of the correlation matrix.</li>
<li><strong>Minimum residuals</strong>: attempts to minimize the residuals of the off-diagonal elements of the correlation matrix, i.e. minimized the difference in the observed versus model implied correlation matrix. This is the default in the <span class="pack">psych</span> package.</li>
<li><strong>Maximum likelihood</strong>: the most widely used approach, assumes a normal distribution for the data generating process, but can be extended to others. <a href="https://github.com/mclark--/Miscellaneous-R-Code/blob/master/ModelFitting/cfa_ml.R">Click here</a> for some by-hand R code with comparison to <span class="pack">lavaan</span> output.</li>
<li><strong>Bayesian</strong>: the basic method extends the maximum likelihood approach as with other analyses by adding priors to the estimation process, i.e. posterior <span class="math inline">\(\propto\)</span> likelihood + prior. A different tact would estimate the factor scores as additional parameters (along with the loadings) etc.</li>
</ul>
</div>
<div id="nonparametric-factor-analysis" class="section level4">
<h4>Nonparametric Factor Analysis</h4>
<p>As noted previously, one of the hard problems in factor analysis is determining the number of factors to retain, a similar issue seen in cluster analysis. Although relatively rarely used, if even heard of, among many practitioners of factor analysis, Bayesian nonparametric approaches get around this by letting the number of clusters/factors grow with the data. Sticking with factor analysis, an underlying Dirichlet or other process is assumed, and depending on the prior, new factors will be added with more or less ease. Such approaches would also be applicable to a streaming data context. See the <a href="bayesian-nonparametric-models.html#bayesian-nonparametric-models">related section</a> in this document.</p>
</div>
</div>
<div id="other-techniques" class="section level3">
<h3>Other Techniques</h3>
<p>Before leaving PCA and common factor analysis for factor analysis of the sort we’ll mostly be concerned with in SEM, I’ll mention other matrix factorization techniques that might be of use depending on your data situation, or merely of interest otherwise. In general, the use of latent variables is wide, and a number of techniques may be more suitable for certain data situations or modeling goals. Just having PCA and Factor Analysis at your disposal may be too limiting. However, many of the techniques are similar to others. For example, many have ties to PCA in various ways, as noted below.</p>
<ul>
<li><strong>SVD</strong>: singular value decomposition. Works on a raw data matrix rather than covariance matrix, and is still a very viable technique that may perform better in a lot of situations relative to fancier latent variable models, and other more recently developed techniques. Variations on SVD are/were behind some recommender systems of the sort you come across at Amazon, Netflix etc. There is a notable <a href="https://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca">tie between SVD and PCA</a> such that SVD is actually used for PCA for computational reasons in many scenarios.</li>
<li><strong>ICA</strong>: Independent components analysis. Extracts non-normal, independent components. The primary goal is to create independent latent variables.</li>
<li><strong>Generalized PCA</strong>: PCA techniques for different types of data, e.g. binary data situations.</li>
<li><strong>PC Regression</strong>: combining PCA with regression in one model. There isn’t a reason I can think of to ever do this relative to alternatives (e.g. SEM, PLS, neural networks, etc.).</li>
<li><strong>NMF</strong>: non-negative matrix factorization. Applied to positive valued matrices, produces positive valued factors. Useful, for example, when dealing with counts.</li>
<li><strong>LSI</strong>: Latent Semantic Indexing, an early form of topic modeling. It has an equivalence to NMF.</li>
<li><strong>LDA</strong>: Latent Dirichlet Allocation, the typical approach for topic modeling, generalizes LSI/NMF. Essentially can be seen as discrete PCA, i.e. a special case of exponential/generalized PCA. See the <a href="topic-models.html#topic-models">topic models</a> section for an example.</li>
<li><strong>Canonical correlation</strong>: an early approach to correlating two sets of variables by constructing composite variables of the corresponding sets. Standard regression, MANOVA/discriminant function analysis, and the simple Pearson correlation are all special cases. Superseded by SEM.</li>
<li>Many others. Check the <a href="latent-variables-1.html#some-other-uses-of-latent-variables">‘other uses’</a> section.</li>
</ul>
</div>
<div id="summary-2" class="section level3">
<h3>Summary</h3>
<p>So to summarize, we have two techniques, very similar on the surface, but with key differences between them. Both allow us to reduce the data to fewer dimensions than we start with. However, both do so in different ways and with different goals in mind. Depending on the specifics of your situation you might prefer either for dimension reduction or construct exploration.</p>
</div>
</div>
<div id="constructs-and-measurement-models" class="section level2">
<h2>Constructs and Measurement Models</h2>
<p>As we have seen, <span class="emph">factor analysis</span> is a general technique for uncovering latent variables within data. While initially one might think it similar to PCA, we saw a key difference in the underlying causal interpretation, which is more explicit in the SEM setting. Now we will move beyond factor analysis as a dimension reduction technique (and fully ‘exploratory’ technique, see below), and instead present it as an approach with a potentially strong theoretical underpinning, and one that can help us assess measurement error, ultimately even leading to regression models utilizing the latent variables themselves.</p>
<p>So let us turn to what are typically called <span class="emph">measurement models</span> within SEM. The underlying model can be thought of as a case in which the observed variables, in some disciplines referred to as <em>indicators</em> (or <em>manifest</em> variables) of the latent construct, are caused by the latent variable. The degree to which the observed variables correlate with one another depends in part on how much of the underlying (presumed) latent variable they reliably measure<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a>.</p>
<p>For each indicator we can think of a regression model as follows, where <span class="math inline">\(\beta_0\)</span> is the intercept and <span class="math inline">\(\lambda\)</span> the regression coefficient that expresses the effect of the latent variable <span class="math inline">\(F\)</span> on the observed variable <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[X = \beta_0 + \lambda F + \epsilon\]</span></p>
<p>We will almost always have multiple indicators, and often multiple latent variables. Some indicators may be associated with multiple factors.</p>
<p><span class="math display">\[\begin{aligned}
X_1 &amp;= \beta_{01} + \lambda_{11} F_1 + \lambda_{21} F_2  + \epsilon\\
X_2 &amp;= \beta_{02} + \lambda_{12} F_1 + \lambda_{22} F_2  + \epsilon\\
X_3 &amp;= \beta_{03} + \lambda_{13} F_1 + \epsilon
\end{aligned}\]</span></p>
<p>It is important to understand this regression model, because many who engage in factor analysis seemingly do not, and often think of it the other way around, where the observed variables cause the latent. In factor analysis in the SEM settings, these <span class="math inline">\(\lambda\)</span> coefficients are typically called <span class="emph">loadings</span> as they were before, but are in fact interpreted as any other regression coefficient- a one unit change in the latent variable results in a <span class="math inline">\(\lambda\)</span> change in the observed variable. Most factor models assume that, controlling for the latent variable, the observed variables are independent (recall our previous discussion on conditional independence in graphical models), though this is sometimes relaxed. If only one factor is associated with an item and does not correlate with any other factors, then we have a simple regression setting where the standardized coefficient is equal to the correlation between the latent variable and the observed.</p>
<p>In addition, as we would in regression we will have residual variance, <span class="math inline">\(\epsilon\)</span> above, i.e. what the latent variable does not account for in the observed variables. This is the SEM counterpart to the uniquenesses we discussed in the standard setting, and is the same as to the the disturbances from our SEM models with fully observed variables. As there, we can think of a latent variable representing all other causes that are not the construct(s) we are positing in the model. This is why they are also sometimes depicted as circles graphically, with an arrow pointing at the observed variable.</p>
<div id="exploratory-vs.confirmatory" class="section level4">
<h4>Exploratory vs. Confirmatory</h4>
<p>An unfortunate and unhelpful distinction in some disciplines is that of <span class="emph">exploratory</span> vs. <span class="emph">confirmatory</span> factor analysis (and even exploratory SEM). In any regression analysis, there is a non-zero correlation between <em>any</em> variable and some target variable. We don’t include everything for theoretical (and even practical) reasons, which is akin to fixing a path coefficient to zero, and here it is no different. Furthermore, most modeling endeavors could be considered exploratory, regardless of how the model is specified. As such, this distinction doesn’t tell us anything about the model, and is thus unnecessary in my opinion.</p>
<p>As an example, in the above equations <span class="math inline">\(X_3\)</span> is not modeled by <span class="math inline">\(F_2\)</span>, which is the same as fixing the <span class="math inline">\(\lambda_{23}\)</span> coefficient for <span class="math inline">\(F_2\)</span> to <span class="math inline">\(0\)</span>. However, that doesn’t tell me whether the model is exploratory or not, and yet that is all the distinction refers to in a practical sense, namely, whether we let all indicators load on all factors or not. An analysis doesn’t <em>necessarily</em> have more theoretical weight, validity, causal efficacy, etc. due to the paths specified, though as we noted before, not having paths does convey causal interpretation.</p>
</div>
<div id="example" class="section level4">
<h4>Example</h4>
<p>Let’s now see a factor analysis from the SEM approach. The motivating example for this section comes from the National Longitudinal Survey of Youth (1997, NLSY97), which investigates the transition from youth to adulthood. For this example, we will investigate a series of questions asked to the participants in 2006 pertaining to the government’s role in promoting well-being. Questions regarded the government’s responsibility for the following: providing jobs for everyone, keeping prices under control, providing health care, providing for elderly, helping industry, providing for unemployed, reducing income differences, providing college financial aid, providing decent housing, protecting the environment. Each item has four values 1:4, which range from ‘definitely should be’ to ‘definitely should not be’<a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a>. We’ll save this for the exercise.</p>
<p>There are also three items regarding their emotional well-being (depression)- how often the person felt down or blue, how often they’ve been a happy person, and how often they’ve been depressed in the last month. These are also four point scales and range from ‘all of the time’ to ‘none of the time’. We’ll use this here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">depressed =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;data/nlsy97_depressedNumeric.csv&#39;</span>)

<span class="kw">library</span>(lavaan)
modelCode =<span class="st"> &quot;</span>
<span class="st">  depressed =~ FeltDown + BeenHappy + DepressedLastMonth</span>
<span class="st">&quot;</span>
famod =<span class="st"> </span><span class="kw">cfa</span>(modelCode, <span class="dt">data=</span>depressed)
<span class="kw">summary</span>(famod, <span class="dt">standardized=</span>T)</code></pre></div>
<pre><code>lavaan (0.5-23.1097) converged normally after  19 iterations

                                                  Used       Total
  Number of observations                          7183        8985

  Estimator                                         ML
  Minimum Function Test Statistic                0.000
  Degrees of freedom                                 0

Parameter Estimates:

  Information                                 Expected
  Standard Errors                             Standard

Latent Variables:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  depressed =~                                                          
    FeltDown          1.000                               0.541    0.813
    BeenHappy        -0.732    0.020  -37.329    0.000   -0.396   -0.609
    DeprssdLstMnth    0.719    0.019   37.992    0.000    0.388    0.655

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .FeltDown          0.150    0.007   20.853    0.000    0.150    0.339
   .BeenHappy         0.266    0.006   46.489    0.000    0.266    0.629
   .DeprssdLstMnth    0.201    0.005   41.606    0.000    0.201    0.571
    depressed         0.292    0.010   30.221    0.000    1.000    1.000</code></pre>
<div id="raw-results" class="section level5">
<h5>Raw results</h5>
<p>In a standard measurement model such as this we must <em>scale the factor</em> by fixing one of the indicator’s loadings to one. This is done for identification purposes, so that we can estimate the latent variable variance, which is otherwise arbitrary (as is the mean, though we usually, but not always, are not concerned about). Which variable is selected for scaling is arbitrary, but doing so means that the sum of the latent variable variance and the residual variance of the variable whose loading is fixed to one equals the variance of that observed variable<a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a>.</p>
<div style="width:50%; margin-left:auto;  margin-right:auto;">
<div id="htmlwidget-2cd8f43a850e049b5c8b" style="width:100%;height:50%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-2cd8f43a850e049b5c8b">{"x":{"diagram":"digraph depression {\n # note that without basically treating each node and edge as a snowflake, most attributes will not be applied \n\n # Intialization of graph attributes\n graph [overlap=false rankdir=TB bgcolor=transparent]\n \n # Node statements\n node [shape=circle, style=filled, fillcolor=lightblue, color=gray75, fontcolor=gray50, fontname=\"Roboto\"];\n U0 [width=.25 height=.25 style=\"\" label=\"\"];\n Depression [fontsize=10, width=1.25, height=1];\n \n node [width=1, height=1, fixedsize=true, shape=square, color=lightblue, style=\"\"]\n subgraph {\n    rank=same;\n    node [label=\"Felt\\n\\nDown\" fontcolor=gray50 fontsize=6]    \n    FD; \n    node [label=\"Been\\n\\nHappy\" fontcolor=gray50 fontsize=6]    \n    BH; \n    node [label=\"Depressed\\n\\nLast Month\" fontcolor=gray50 fontsize=6]    \n    DLM; \n }\n # Initialization of edge attributes\n edge [color = gray50, rel = yields, fontname=\"Roboto\"]\n \n # Edge statements\n edge [label=\"   .29\", fontcolor=gray25, fontsize=6]\n U0 -> Depression;\n edge [style=dashed label=\"   1.0\" fontcolor=gray25 fontsize=10 minlen=2]\n Depression -> FD \n edge [style=\"\" label=\"   -.73\" fontcolor=gray25 fontsize=10 minlen=2];\n Depression -> BH ;\n edge [label=\"   .72\" fontcolor=gray25 fontsize=10 minlen=2]\n Depression -> DLM ;\n\n subgraph {\n    rank=same;\n    node [width=.25, height=.25, shape=circle, color=gray80]\n    U1 [label=\"\"]; \n    U2 [label=\"\"]; \n    U3 [label=\"\"]; \n }\n\n  edge [dir=back fontsize=6 fontcolor=gray25 minlen=.5] \n  FD -> U1 [label=\"   .15\"];\n  BH -> U2 [label=\"   .27\"];\n  DLM -> U3 [label=\"   .20\"];\n }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(depressed<span class="op">$</span>FeltDown, <span class="dt">na.rm=</span>T)  <span class="co"># .29 + .15</span></code></pre></div>
<pre><code>[1] 0.441856</code></pre>
</div>
<div id="standardized-latent-variable" class="section level5">
<h5>Standardized latent variable</h5>
<p>An alternative way to scale the latent variable is to simply fix its variance to one (the <code>std.lv=TRUE</code> results). It does not need to be estimated, allowing us to obtain loadings for each observed variable. Again, think of the SLiM setting. The loadings would be standardized coefficients where the latent construct is the standardized covariate predicting the item of interest.</p>
</div>
<div id="standardized-latent-and-observed" class="section level5">
<h5>Standardized latent and observed</h5>
<p>With both standardized (using the <span class="func">summary</span> function, set <code>standardized=T</code>), these loadings represent correlations between the observed and latent variables. This is the default output in the factor analysis we’d get from non-SEM software (i.e. ‘exploratory’ FA). If one is just doing a factor-analytic model, these loadings are typically reported. Standardized coefficients in a CFA are computed by taking the unstandardized coefficient (loading) and multiplying it by the model implied standard deviation of the indicator then dividing by the latent variable’s standard deviation. Otherwise, one can simply use standardized variables in the analysis, or supply only the correlation matrix.</p>
<div style="width:50%; margin-left:auto;  margin-right:auto;">
<div id="htmlwidget-8f2e10b056f3ea701f34" style="width:100%;height:50%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-8f2e10b056f3ea701f34">{"x":{"diagram":"digraph depression_std {\n # note that without basically treating each node and edge as a snowflake, most attributes will not be applied \n\n # Intialization of graph attributes\n graph [overlap=false rankdir=TB bgcolor=transparent]\n \n # Node statements\n node [shape=circle, style=filled, fillcolor=lightblue, color=gray75,  fontcolor=gray50, fontname=\"Roboto\"];\n U0 [width=.25 height=.25 style=\"\" label=\"\"];\n Depression [fontsize=10, width=1.25, height=1];\n \n node [width=1, height=1, fixedsize=true, shape=square, color=lightblue, style=\"\"]\n subgraph {\n    rank=same;\n    node [label=\"Felt\\n\\nDown\" fontcolor=gray50 fontsize=6]    \n    FD; \n    node [label=\"Been\\n\\nHappy\" fontcolor=gray50 fontsize=6]    \n    BH; \n    node [label=\"Depressed\\n\\nLast Month\" fontcolor=gray50 fontsize=6]    \n    DLM; \n }\n # Initialization of edge attributes\n edge [color = gray50, rel = yields, fontname=\"Roboto\"]\n \n # Edge statements\n edge [style=dashed label=\"   1.0\", fontcolor=gray25, fontsize=6]\n U0 -> Depression;\n edge [label=\"   .81\" fontcolor=gray25 fontsize=10 minlen=2]\n Depression -> FD \n edge [style=\"\" label=\"   -.61\" fontcolor=gray25 fontsize=10 minlen=2];\n Depression -> BH ;\n edge [label=\"   .65\" fontcolor=gray25 fontsize=10 minlen=2]\n Depression -> DLM ;\n\n subgraph {\n    rank=same;\n    node [width=.25, height=.25, shape=circle, color=gray80]\n    U1 [label=\"\"]; \n    U2 [label=\"\"]; \n    U3 [label=\"\"]; \n }\n\n  edge [dir=back fontsize=6 fontcolor=gray25 minlen=.5] \n  FD -> U1 [label=\"   .34\"];\n  BH -> U2 [label=\"   .63\"];\n  DLM -> U3 [label=\"   .57\"];\n }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
<div id="multiple-factors-in-sem" class="section level4">
<h4>Multiple factors in SEM</h4>
<p>When we move to multiple factors, not much changes relative to what we saw in the non-SEM setting and with the one factor case in the SEM. We still specify specific paths, and we are now interested in the correlation between the two latent variables.</p>
<p>The following data from the <span class="pack">psych</span> package contain 25 personality items regarding the <a href="https://en.wikipedia.org/wiki/Big_Five_personality_traits">Big 5</a> scales. There are 2800 total subjects, and five categories of questions[^big5].</p>
<ul>
<li>agreeableness: a tendency to be compassionate and cooperative rather than suspicious and antagonistic towards others</li>
<li>conscientiousness: a tendency to be organized and dependable, show self-discipline, planning</li>
<li>openness to experience: appreciation for art, emotion, adventure, unusual ideas, curiosity, and variety of experience.</li>
<li>extraversion: energy, assertiveness, sociability and the tendency to seek stimulation in the company of others, and talkativeness.</li>
<li>neuroticism: prone to physiological stress, tendency to experience unpleasant emotions easily, such as anger, anxiety, depression, and vulnerability</li>
</ul>
<p>We’ll just look at part of this to keep things simple, namely the agreeableness and neuroticism items. We’ll posit two latent constructs, and we assume them to be correlated. Note that by default, most if not all SEM programs assume you want the latent variables to be correlated. In general it would be useful to compare the fit of a model with and without the correlation regardless of what theory dictates. Since item A1 is reverse scored, I make A2 the scaling variable to keep the loadings mostly positive.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bfi_trim =<span class="st"> </span>bfi <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">matches</span>(<span class="st">&#39;A[1-5]|N[1-5]&#39;</span>))
modelCode =<span class="st"> &quot;</span>
<span class="st">  agree =~ A2 + A1 + A3 + A4 + A5</span>
<span class="st">  neurot =~ N1 + N2 + N3 + N4 + N5</span>

<span class="st">  # not necessary</span>
<span class="st">  # agree ~~ neurot</span>

<span class="st">  # if you don&#39;t want them correlated</span>
<span class="st">  # agree ~~ 0*neurot</span>
<span class="st">&quot;</span>

cfa_model =<span class="st"> </span><span class="kw">cfa</span>(modelCode, <span class="dt">data=</span>bfi_trim)
<span class="kw">summary</span>(cfa_model, <span class="dt">standardized=</span>T, <span class="dt">rsq=</span>T, <span class="dt">nd=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>lavaan (0.5-23.1097) converged normally after  33 iterations

                                                  Used       Total
  Number of observations                          2618        2800

  Estimator                                         ML
  Minimum Function Test Statistic              694.620
  Degrees of freedom                                34
  P-value (Chi-square)                           0.000

Parameter Estimates:

  Information                                 Expected
  Standard Errors                             Standard

Latent Variables:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  agree =~                                                              
    A2                 1.00                                0.77     0.65
    A1                -0.71     0.04   -16.53     0.00    -0.54    -0.39
    A3                 1.30     0.05    26.43     0.00     0.99     0.76
    A4                 0.95     0.05    20.15     0.00     0.73     0.49
    A5                 1.06     0.04    24.74     0.00     0.81     0.64
  neurot =~                                                             
    N1                 1.00                                1.29     0.82
    N2                 0.95     0.02    40.79     0.00     1.23     0.80
    N3                 0.88     0.02    36.57     0.00     1.14     0.71
    N4                 0.68     0.02    27.85     0.00     0.88     0.56
    N5                 0.63     0.03    24.59     0.00     0.81     0.50

Covariances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  agree ~~                                                              
    neurot            -0.22     0.03    -8.66     0.00    -0.22    -0.22

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .A2                 0.80     0.03    27.60     0.00     0.80     0.58
   .A1                 1.67     0.05    34.21     0.00     1.67     0.85
   .A3                 0.73     0.04    20.61     0.00     0.73     0.42
   .A4                 1.69     0.05    32.67     0.00     1.69     0.76
   .A5                 0.94     0.03    28.11     0.00     0.94     0.59
   .N1                 0.79     0.04    22.09     0.00     0.79     0.32
   .N2                 0.83     0.03    23.88     0.00     0.83     0.35
   .N3                 1.25     0.04    29.35     0.00     1.25     0.49
   .N4                 1.69     0.05    33.28     0.00     1.69     0.69
   .N5                 1.97     0.06    34.08     0.00     1.97     0.75
    agree              0.59     0.04    16.10     0.00     1.00     1.00
    neurot             1.67     0.07    23.70     0.00     1.00     1.00

R-Square:
                   Estimate
    A2                 0.42
    A1                 0.15
    A3                 0.58
    A4                 0.24
    A5                 0.41
    N1                 0.68
    N2                 0.65
    N3                 0.51
    N4                 0.31
    N5                 0.25</code></pre>
<p>To begin, we do see that the factors are correlated at roughly -0.22, but some items are not loading too highly on their respective construct, at least for Agreeableness, and some R-squares reinforce that notion. Remember also our distinction between pattern and structure coefficients though. Just because we don’t have paths from some items to the other factor doesn’t mean they aren’t correlated with that factor.</p>
<div style="width:100%; margin:auto auto;">
<div id="htmlwidget-d62cff6aceee5f4f4785" style="width:100%;height:500px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-d62cff6aceee5f4f4785">{"x":{"diagram":"digraph multifactor {\n # note that without basically treating each node and edge as a snowflake, most attributes will not be applied \n\n # Intialization of graph attributes\n graph [overlap=false rankdir=TB bgcolor=transparent]\n \n # Node statements\n node [shape=circle, style=filled, fillcolor=lightblue, color=gray75,  fontcolor=gray50];\n \n subgraph {\n rank = same;\n Agree [fontsize=10, width=1.25, height=1, fillcolor=palegreen];\n Neurot [fontsize=10, width=1.25, height=1, fillcolor=salmon];\n }\n\n node [width=1, height=1, fixedsize=true, shape=square, style=\"\"]\n subgraph {\n    rank=same;\n    node [fontcolor=gray50 fontsize=10]\n    node [color=palegreen]\n    A1 A2 A3 A4 A5; \n    node [color=salmon]\n    N1 N2 N3 N4 N5; \n }\n # Initialization of edge attributes\n edge [color = gray50, rel = yields]\n \n # Edge statements\n \n edge [label=\"   -.22\" fontcolor=gray25 fontsize=10 minlen=2 dir=both]\n Agree -> Neurot;\n\n edge [label=\"  -.39\" fontcolor=gray25 fontsize=10 minlen=2 dir=forward]\n Agree -> A1; \n edge [label=\"   .65\" fontcolor=gray25 fontsize=10 minlen=2]\n Agree -> A2;\n edge [label=\"   .76\" fontcolor=gray25 fontsize=10 minlen=2]\n Agree -> A3;\n edge [label=\"   .49\" fontcolor=gray25 fontsize=10 minlen=2]\n Agree -> A4;\n edge [label=\"   .67\" fontcolor=gray25 fontsize=10 minlen=2]\n Agree -> A5;\n edge [label=\"   .82\" fontcolor=gray25 fontsize=10 minlen=2]\n Neurot -> N1; \n edge [label=\"   .80\" fontcolor=gray25 fontsize=10 minlen=2]\n Neurot -> N2;\n edge [label=\"   .71\" fontcolor=gray25 fontsize=10 minlen=2];\n Neurot -> N3;\n edge [label=\"   .56\" fontcolor=gray25 fontsize=10 minlen=2]\n Neurot -> N4;\n edge [label=\"   .50\" fontcolor=gray25 fontsize=10 minlen=2]\n Neurot -> N5;\n\n subgraph {\n    rank=same;\n    node [width=.25, height=.25, shape=circle, color=gray80]\n    U1 [label=\"\"]; \n    U2 [label=\"\"]; \n    U3 [label=\"\"]; \n    U4 [label=\"\"]; \n    U5 [label=\"\"]; \n    U6 [label=\"\"]; \n    U7 [label=\"\"]; \n    U8 [label=\"\"]; \n    U9 [label=\"\"]; \n    U10 [label=\"\"]; \n }\n\n  edge [dir=back fontsize=8 fontcolor=gray25 minlen=.5] \n  A1 -> U1 [label=\"   .58\"];\n  A2 -> U2 [label=\"   .85\"];\n  A3 -> U3 [label=\"   .42\"];\n  A4 -> U4 [label=\"   .76\"];\n  A5 -> U5 [label=\"   .59\"];\n  N1 -> U6 [label=\"   .32\"];\n  N2 -> U7 [label=\"   .35\"];\n  N3 -> U8 [label=\"   .49\"];\n  N4 -> U9 [label=\"   .69\"];\n  N5 -> U10 [label=\"   .75\"];\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
<div id="other-issues-in-factor-analysis" class="section level2">
<h2>Other issues in Factor Analysis</h2>
<div id="some-specific-factor-models-in-sem" class="section level3">
<h3>Some specific factor models in SEM</h3>
<div id="hierarchicalsecond-order" class="section level4">
<h4>Hierarchical/Second-Order</h4>
<p>In <span class="emph">hierarchical</span> models we posit that latent variables can also serve as indicators of other latent variables. As an example, one latent variable might represent verbal cognition, another mathematical strength, a third scientific acumen, and they might be indicators for a general ‘scholastic ability’ latent variable. Graphically they can be depicted as follows:</p>
<div style="width:75%; margin:0 auto; text-align=&#39;center&#39;">
<div id="htmlwidget-14a82e1654beb4fcedb9" style="width:100%;height:45%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-14a82e1654beb4fcedb9">{"x":{"diagram":"digraph secondorder {\n # Intialization of graph attributes\n graph [overlap = false rankdir=TB bgcolor=transparent]\n \n # Initialization of node attributes\n node [shape = circle,\n fontname = Helvetica,\n color = gray80,\n type = box,\n fixedsize = true]\n \n # Node statements\n node [width=1.5, height=1, shape=circle, fontcolor=gray25];\n  F1 [label=<Factor<sub>1<\/sub>>, color=\"#ff5503\"]; \n  F2 [label=<Factor<sub>2<\/sub>>, color=salmon]; \n  F3 [label=<Factor<sub>3<\/sub>>, color=limegreen]; \n  Fg [label=<Factor<sub>g<\/sub>>, color=dodgerblue]; \n \n node [width=1, shape=square, color=gray10]\n subgraph {\n    rank=same;\n    X1 [label=<X<sub>1<\/sub>>, color=\"#ff5503\"]; \n    X2 [label=<X<sub>2<\/sub>>, color=\"#ff5503\"]; \n    X3 [label=<X<sub>3<\/sub>>, color=\"#ff5503\"]; \n    X4 [label=<X<sub>4<\/sub>>, color=\"#ff5503\"]; \n }\n node [width=1, shape=square, color=salmon]\n subgraph {\n    rank=same;\n    X5 [label=<X<sub>5<\/sub>>]; \n    X6 [label=<X<sub>6<\/sub>>]; \n    X7 [label=<X<sub>7<\/sub>>]; \n    X8 [label=<X<sub>8<\/sub>>]; \n }\n node [width=1, shape=square, color=limegreen]\n subgraph {\n    rank=same;\n    X9 [label=<X<sub>9<\/sub>>]; \n    X10 [label=<X<sub>10<\/sub>>]; \n    X11 [label=<X<sub>11<\/sub>>]; \n    X12 [label=<X<sub>12<\/sub>>]; \n }\n # Initialization of edge attributes\n edge [color = gray50, rel = yields]\n \n # Edge statements\n F1 -> X1 [style=dashed color=\"#ff5503\"] ;\n F1 -> {X2 X3 X4} [color=\"#ff5503\"] ;\n\n F2 -> X5 [style=dashed  color=salmon] ;\n F2 -> {X6 X7 X8} [color=salmon];\n\n F3 -> X9 [style=dashed color=limegreen] ;\n F3 -> {X10 X11 X12}[color=limegreen];\n\n Fg -> F1 [style=dashed color=dodgerblue] ;\n Fg -> {F2 F3} [color=dodgerblue];\n\n \n }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<p>The above is equivalent to a three factor model with correlated factors. This is demonstrated with the following.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">HS.model =<span class="st">  &#39; visual  =~ x1 + x2 + x3 </span>
<span class="st">              textual =~ x4 + x5 + x6</span>
<span class="st">              speed   =~ x7 + x8 + x9 &#39;</span>
HS.model2 =<span class="st"> &#39; visual  =~ x1 + x2 + x3 </span>
<span class="st">              textual =~ x4 + x5 + x6</span>
<span class="st">              speed   =~ x7 + x8 + x9 </span>
<span class="st">              g =~ visual + textual + speed &#39;</span>
fit1 =<span class="st"> </span><span class="kw">cfa</span>(HS.model, <span class="dt">data =</span> HolzingerSwineford1939)
fit2 =<span class="st"> </span><span class="kw">cfa</span>(HS.model2, <span class="dt">data =</span> HolzingerSwineford1939)
## summary(semTools::compareFit(fit1, fit2))</code></pre></div>
<table>
<colgroup>
<col width="9%" />
<col width="9%" />
<col width="5%" />
<col width="14%" />
<col width="10%" />
<col width="10%" />
<col width="8%" />
<col width="8%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="left">chisq</th>
<th align="left">df</th>
<th align="left">pvalue</th>
<th align="left">cfi</th>
<th align="left">tli</th>
<th align="left">aic</th>
<th align="left">bic</th>
<th align="left">rmsea</th>
<th align="left">srmr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">fit1</td>
<td align="left">85.31</td>
<td align="left">24</td>
<td align="left">8.503e-09</td>
<td align="left">0.9306</td>
<td align="left">0.8958</td>
<td align="left">7517</td>
<td align="left">7595</td>
<td align="left">0.09212</td>
<td align="left">0.06521</td>
</tr>
<tr class="even">
<td align="left">fit2</td>
<td align="left">85.31</td>
<td align="left">24</td>
<td align="left">8.503e-09</td>
<td align="left">0.9306</td>
<td align="left">0.8958</td>
<td align="left">7517</td>
<td align="left">7595</td>
<td align="left">0.09212</td>
<td align="left">0.06521</td>
</tr>
</tbody>
</table>
<p>However, theoretically we may have a specific latent cause in mind and want to express it in this hierarchical manner. In other cases, model complexity necessitates the hierarchical approach.</p>
</div>
<div id="bifactor" class="section level4">
<h4>Bifactor</h4>
<p>A <span class="emph">bifactor</span> model might seem similar to the hierarchical model at first blush. I’ll use the previous 3-factor model example, and see if you can come up with why these are different models specifically.</p>
<div style="width:75%; margin:0 auto; text-align=&#39;center&#39;">
<div id="htmlwidget-f2857d0adc2890314bb4" style="width:100%;height:45%;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-f2857d0adc2890314bb4">{"x":{"diagram":"digraph bifactor {\n # Intialization of graph attributes\n graph [overlap = false rankdir=BT bgcolor=transparent]\n \n # Initialization of node attributes\n node [shape = circle,\n fontname = Helvetica,\n color = gray80,\n type = box,\n fixedsize = true]\n \n # Node statements\n node [width=1.5, height=1, shape=circle, fontcolor=gray25];\n subgraph{rank=same;\n  F1 [label=<Factor<sub>1<\/sub>>, color=\"#ff5500\"]; \n  F2 [label=<Factor<sub>2<\/sub>>, color=salmon]; \n  F3 [label=<Factor<sub>3<\/sub>>, color=limegreen]; \n  }\n  Fg [label=<Factor<sub>g<\/sub>>, color=dodgerblue]; \n \n node [width=1, shape=square, color=gray10]\n subgraph {\n    rank=same;\n    X1 [label=<X<sub>1<\/sub>>, color=\"#ff5500\"]; \n    X2 [label=<X<sub>2<\/sub>>, color=\"#ff5500\"]; \n    X3 [label=<X<sub>3<\/sub>>, color=\"#ff5500\"]; \n    X4 [label=<X<sub>4<\/sub>>, color=\"#ff5500\"]; \n }\n node [width=1, shape=square, color=salmon]\n subgraph {\n    rank=same;\n    X5 [label=<X<sub>5<\/sub>>]; \n    X6 [label=<X<sub>6<\/sub>>]; \n    X7 [label=<X<sub>7<\/sub>>]; \n    X8 [label=<X<sub>8<\/sub>>]; \n }\n node [width=1, shape=square, color=limegreen]\n subgraph {\n    rank=same;\n    X9 [label=<X<sub>9<\/sub>>]; \n    X10 [label=<X<sub>10<\/sub>>]; \n    X11 [label=<X<sub>11<\/sub>>]; \n    X12 [label=<X<sub>12<\/sub>>]; \n }\n # Initialization of edge attributes\n edge [color = gray50, rel = yields]\n \n # Edge statements\n F1 -> X1 [style=dashed color=\"#ff5500\"] ;\n F1 -> {X2 X3 X4} [color=\"#ff5500\"] ;\n\n F2 -> X5 [style=dashed  color=salmon] ;\n F2 -> {X6 X7 X8} [color=salmon];\n\n F3 -> X9 [style=dashed color=limegreen] ;\n F3 -> {X10 X11 X12}[color=limegreen];\n\n # edge[dir=both, style=curved]\n # F1 -> F2;\n # F2 -> F3;\n # F3 -> F1;\n\n edge [dir=back; minlen=2.3 color=dodgerblue]\n X1 -> Fg [style=dashed]; \n {X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12} -> Fg;\n\n \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<p>The gist is that now the general factor has no correlation to the others, and the specific factors <span class="math inline">\(F_1\)</span>, <span class="math inline">\(F_2\)</span> and <span class="math inline">\(F_3\)</span> account for whatever it does not. Disturbances for the factors 1-3 in the hierarchical model are now the latent factors 1-3 in the bifactor model. For more on these models and reliability statistics based on them, I highly recommend taking a look at the <span class="func">omega</span> function in the <span class="pack">psych</span> package. It may surprise some that there are actually measures of reliability that are not called Cronbach’s alpha that have been developed since 1950. Speaking of which, see the <a href="latent-variables-1.html#scale-development">scale development</a> section below.</p>
</div>
<div id="measurement-invariance" class="section level4">
<h4>Measurement invariance</h4>
<p>Measurement invariance refers generally to consistency of a construct under different settings. Unfortunately there is not one single way we can define ‘invariance’, and testing for more strict settings would often posit implausible models (my opinion). One can think of it as taking a multi-group approach in which the factor analysis is conducted separately for each group, e.g. male vs. female, but where we constrain various parameters to be equal (or not). A starting point is simply to determine whether the same factor structure holds, e.g. a one factor model in each setting. This is referred to as configural invariance. The common types of measurement invariance include the following list from least restrictive to most. Each assumes the prior setting, so if a less restrictive setup does not hold, there is no point in doing further tests.</p>
<ol style="list-style-type: decimal">
<li><span class="emph">Configural</span>: The same FA model is specified for each group.</li>
<li><span class="emph">Weak</span>: Requires equal unstandardized pattern coefficients. Rejection implies the latent variables have different meanings across groups or perhaps there is a different style of response to the items.</li>
<li><span class="emph">Strong</span>: Requires equal unstandardized intercepts for indicators. Rejection suggestion other variables unrelated to the latent variable produce a different pattern of response (Kline refers to this as differential additive response style)</li>
<li><span class="emph">Strict</span>: Includes equal (co-)variances. Required for a claim of <em>identical</em> latent structure.</li>
</ol>
<p>Testing for invariance involves inclusion of mean structures in the model and specific constraints pertaining to the type of invariance tested. There are numerous issues with the approach. One problem is that this necessitates more data than most seem to have that want to do this. Further, it potentially assumes an interaction with the group variable for <em>every</em> parameter that might be estimated. This is implausible in my opinion, and we rarely assume this elsewhere in standard model settings. Conversely, I can’t think of a viable theoretical reason to assume, or even want, that every parameter would be the same across groups outside of a formal testing situation (e.g. scholastic exams). So one is left with some sort mix and match approach or similar to guess what may or may not vary. Yet one more issue is statistical. Let’s say we want to test loading equivalence across groups. You find a global test would suggest that they are not equal. However, inspection of the interval estimates for each loading might suggest considerable overlap. And finally, I’ve not seen any sort of regularization applied to testing all these parameters, which would typically be several dozen parameters even in the simplest of settings.</p>
<p>If you want to examine group specific output, it’s no change for the <span class="pack">lavaan</span> approach, you simply add the argument <code>group='x'</code> to get results for multiple groups for grouping variable ‘x’. In the <span class="pack">semTools</span> package, the <span class="func">measurementInvariance</span> function works in the exact same way, but returns the statistical test for each of the versions of invariance mentioned above, plus another that restricts the means as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cfa</span>(HS.model, <span class="dt">data=</span>HolzingerSwineford1939, <span class="dt">group=</span><span class="st">&quot;school&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summary</span>(<span class="dt">ci=</span>T)
semTools<span class="op">::</span><span class="kw">measurementInvariance</span>(HS.model, <span class="dt">data=</span>HolzingerSwineford1939, <span class="dt">group=</span><span class="st">&quot;school&quot;</span>, <span class="dt">strict=</span>T)</code></pre></div>
<p>Some also refer to <span class="emph">stability</span> or measurement invariance over time in the longitudinal setting. This in fact important to test, but difficult when there is drop out. For example, is it that the construct changes or simply you aren’t able to capture the same individuals (even with some sort of imputation approach)? Furthermore, if there is a large separation in time, it may be possible to get similar results but the constructs are in fact different, a form of the naming fallacy problem discussed previously.</p>
<p>The <span class="pack">semTools</span> package also has a <span class="func">longInvariance</span> for the longitudinal setting.</p>
</div>
</div>
<div id="scale-development" class="section level3">
<h3>Scale development</h3>
<p>A good use of factor analysis regards scale development. If we come up with 10 items that reflect some underlying construct, factor analysis can provide a sense of how well the scale is put together. Recall that in path analysis, residual variance, sometimes called disturbance<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a>, reflects both omitted causes as well as measurement error. In this context, <span class="math inline">\(1-R^2_{item}\)</span> provides a sense of how unreliable the item is. A perfectly reliable item would be perfectly explained by the construct it is a measure of. Strong loadings indicate a strong relationship between the item and the underlying construct.</p>
<p>The following diagram shows how the variance breaks down (from Kline).</p>
<p><img src="img/uniquevcommon.png" style="display:block; margin: 0 auto; width:50%"></p>
<div id="reliability" class="section level4">
<h4>Reliability</h4>
<p>Please repeat the following 10 times, possibly while in the lotus position.</p>
<blockquote>
<p>Reliability is not a constant. There is no single concept of reliability.</p>
</blockquote>
<p>As a motivating example, say we derive a scale for ‘stick-to-it-tiveness’ and did so based on a largely white, young adult, somewhat educated, middle class sample of Americans<a href="#fn40" class="footnoteRef" id="fnref40"><sup>40</sup></a>. Let’s say the reliability for the scale was off the charts based on whatever statistic you want to use, and you are most pleased. Now answer the following…</p>
<p>How reliable do you think the scale is on a sample of:</p>
<ul>
<li><p>native Chinese speakers?</p></li>
<li><p>a sample of Londoners?</p></li>
<li><p>a sample of older adults from the Texas panhandle?</p></li>
<li><p>a sample of women only?</p></li>
<li><p>a sample of second generation Hispanic Americans?</p></li>
<li><p>a sample of teenagers?</p></li>
<li><p>the same population 15 years later?</p></li>
<li><p>the same exact people a second time?</p></li>
</ul>
<p>Asking it in this context makes it almost seem absurd to even think the scale would be equally effective for all these groups. While some scales are in fact applicable across a wide spectrum of individuals, assuming it to be reliable for all scenarios is at best a problematic stance. Yet common practice for utilizing scales is to simply cite the original study, possibly conducted <em>decades</em> ago, note that the scale was found to be ‘reliable’, possibly without even defining reliability or producing any statistic, and going on with using the scale. Furthermore, many adhere to sometimes very low, and utterly arbitrary, statistical cutoffs, largely on a single type of statistic that may not even be appropriate given the construct(s) under study.</p>
<p>Let’s be more explicit with our example. The scale, which actually measures two aspects of stick-to-it-tiveness is developed on mostly freshman psychology students at a private school, and the Cronbach’s alpha for it is .7. To begin, even if we deem this an acceptable result, there is no a priori reason to think it will generalize to even a more equitable population of college students at the same school, much less adults out of this age range or any number of other specific populations we could think of. In addition, there is a notable portion of the variance of the items that isn’t related to the construct, and one wonders whether that is in fact a good result. And finally, our statistic of choice is actually a bit problematic if there is no general factor beyond the two aspects.</p>
<div id="classical-test-theory" class="section level5">
<h5>Classical Test Theory</h5>
<p>The underlying idea behind many measures of reliability is the following simple equation:</p>
<p><span class="math display">\[O = T + E\]</span></p>
<p>In words, the observed score equals the true score plus measurement error. One of the key consequences of measurement error is attenuation of correlation. If <span class="math inline">\(\mathcal{X^*}\)</span> and <span class="math inline">\(\mathcal{Y^*}\)</span> are the true scores, and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the ones we actually observe and contain measurement error, <span class="math inline">\(\mathrm{cor}_{\mathcal{X^*Y^*}}\)</span> will always be greater than <span class="math inline">\(\mathrm{cor}_{XY}\)</span>, and using less reliable measures can only hurt any analysis that seeks to find correlations among the data (i.e. <em>every</em> analysis). Examining a scale’s reliability is an attempt to understand the measurement error involved. Even if you aren’t working with multi-item scales, the problem doesn’t go away, and you should think hard about how accurately your variables represent what they intend to.</p>
</div>
<div id="measures-of-reliability" class="section level5">
<h5>Measures of Reliability</h5>
<p>There are many, many measures of reliability. Here are a couple.</p>
<ul>
<li><strong>Parallel Measures/Split-half</strong>: Many reliability measures rely on the notion of parallel tests, such that reliability can be seen the correlation between two parallel forms of a test. More explicitly, consider a single test with items randomly split into two sets. The correlation between those two tests can be seen as a measure of reliability.</li>
<li><strong>Guttman’s <span class="math inline">\(\lambda_6\)</span></strong>: is based on the R<sup>2</sup> of each item regressed on the others, and this squared multiple correlation can be seen as a lower bound on the communality. The <span class="math inline">\(\lambda_6\)</span> can be seen as a lower bound on split-half reliability.</li>
<li><strong>Greatest lower bound</strong>: Multiple versions. One is the greatest split half reliability (essentially Guttman’s <span class="math inline">\(\lambda_4\)</span>). Another is conceptually 1 - error/total variance, similar to <span class="math inline">\(\omega_{\mathrm{t}}\)</span> but will actually not necessarily be as ‘great’ as <span class="math inline">\(\omega_{\mathrm{t}}\)</span>.</li>
<li><strong><span class="math inline">\(\beta\)</span></strong>: based on the average correlation among worst split that produces the least inter-item correlations. Somewhat improves on <span class="math inline">\(\alpha\)</span> as a measure of factor saturation but others do better in that regard.</li>
<li><strong>Cronbach’s <span class="math inline">\(\alpha\)</span></strong>: the most widely used/abused measure of reliability. Equivalent to the earlier measure by Guttman (<span class="math inline">\(\lambda_3\)</span>), and it is a generalization of measure for dichotomous items developed in 1937 (<span class="math inline">\(\mathrm{KR}_{20}\)</span>). Issues: underestimates reliability and overestimates factor saturation. Not so useful for multidimensional scales.</li>
<li><strong><span class="math inline">\(\omega\)</span>*</strong>: from McDonald, a measure of ‘factor saturation’. <span class="math inline">\(\omega_{\mathrm{t}}\)</span>, or omega total, provides a proportion of variance that is not unique in the factor-analytic sense. <span class="math inline">\(\omega_{\mathrm{h}}\)</span>, or omega-hierarchical, is the measure associated with the ‘general’ factor in the bifactor model noted above. It assumes enough items for a potential hierarchical structure to be tested in the first place.</li>
<li><strong>Composite Reliability</strong>: described in Kline, reference Raykov (2004), see <span class="math inline">\(\omega\)</span></li>
<li><strong>Average Variance Extracted</strong>: For single factor indicator settings, this is the average squared (standardized) loading. It’s simple and has a straightforward interpretation as R<sup>2</sup> does in regression.</li>
</ul>
<p>Both Guttman’s and Cronbach’s measures are a function of the number of items and average correlation of items, and will increase if either of those increase all else being equal. Neither should be seen as indicating a single factor structure (they assume it). My own opinion is that the most sensical notion of reliability is a function of some underlying model that can actually be tested, i.e. a latent variable model. Furthermore, a good item is infused with the construct it purports to measure. McDonald’s <span class="math inline">\(\omega_t\)</span> gets at that notion explicitly. Practically speaking, if loadings are equal and the one is dealing with a unidimensional construct <span class="math inline">\(\alpha=\omega_t=\omega_h\)</span>. Otherwise, i.e. the usual case, one should consider which measure might be best. The ones with a * are more explicitly driven by an underlying factor model.</p>
<p>Some suggest <strong>AVE</strong> can be used as a measure of <span class="emph">convergent</span> validity (if it’s above some arbitrary value) or <span class="emph">discriminant</span> validity (if it’s square root is higher than the factor’s correlation with some other latent variable). I find this interpretation somewhat problematic. Convergent and discriminant validity have typically always been explicitly defined by a construct’s correlations with <em>other</em> constructs/measures that are similar/different, not individual items, which are theoretically just a random sample of all available items that measure all facets of the construct. If your items, say for a newly proposed depression scale, correlate with one another, that’s all well and good, but it doesn’t mean your scale as a whole automatically measures what you think it does, and would have the proper correlations with other measures, e.g. positive with another established depression measure or anxiety measure, and negative with one measuring self-esteem. You actually have to collect these other measures to establish it. As another example, a poor <strong>AVE</strong>, i.e. one that would not suggest even construct validity (say ave = .1, sqrt = .32), could signify discriminant validity if using the proposed yardstick (&gt;.30 correlation with another factor). It also works in the opposite direction, your very high AVE may be higher than something else the construct strongly correlates with, <em>but shouldn’t</em>.</p>
<p>However, the <strong>AVE</strong> is clearly related to the notion of <span class="emph">construct</span> validity in general, i.e. whether the items measure what they purport to measure, and would be a useful first step in that regard. If one collects other measures they might then test for convergent and discriminant validity, further establishing construct validity.</p>
<p>In the <span class="pack">psych</span> package you can use functions <span class="func">splitHalf</span>, <span class="func">alpha</span>, and <span class="func">omega</span> to get these and other measures, though some are only of historical interest. If you are going to report With <span class="pack">semTools</span>, use the <span class="func">reliability</span> function on a <span class="objClass">lavaan</span> class object. See example below. Because of the amount of output, I only show the results for the last line.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## omega(cog_ability, nfactors=4)
## alpha(scale(Harman.5))
## 
## library(semTools)
## reliability(famod) # omega1 is the omega total, omega 3 is omega hierarchical</code></pre></div>
<div id="htmlwidget-5d33cdbdd6ad96c33ec1" style="width:50%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-5d33cdbdd6ad96c33ec1">{"x":{"filter":"none","data":[["alpha","omega","omega2","omega3","avevar"],["0.61","0.62","0.62","0.62","0.30"],["0.53","0.55","0.55","0.55","0.26"],["0.62","0.66","0.66","0.64","0.27"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>verbal<\/th>\n      <th>spatial<\/th>\n      <th>total<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","sort":false,"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]},"selection":{"mode":"multiple","selected":null,"target":"row"}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="scale-development-as-software-development" class="section level4">
<h4>Scale development as software development</h4>
<p>Scale development is hard and a scale can always be improved, <em>always</em>. There is practically no chance that the first time one comes up with variables to measure a construct that those indicators will be perfect. For example, in a scale that measures some internal construct, such as some aspect of personality, things like item wording, difficulty, context etc. all factor into how the variables will be perceived by the individual, whose own context and background will also affect how items are answered. After analysis, items might be dropped, altered, or new items added. The same goes for non-human constructs, like a democracy scale for countries or a diversity index for species. For example, even economic indicators are not going to have the same reliability from country to country, as the raw data may be harder to come by or not even applicable.</p>
<p>Given this, it would be best to think of scale development the same as we do software development. In software development, bugs are discovered, applications to other or new versions of operating systems are constructed and so forth. The development cycle is continuous and ongoing, or else the app dies as people move on to better products.</p>
<p>With scales, items are discovered to be problematic, applications to new segments of the population are to be developed etc. As such, it would be nice if scales were developed more regularly and came with version numbers, same as software, and not so broadly applied to very different populations. At least this way, perhaps people wouldn’t treat the scales so precious as to not be amenable to any change, and expect more from the development process<a href="#fn41" class="footnoteRef" id="fnref41"><sup>41</sup></a>.</p>
</div>
</div>
<div id="factor-scores" class="section level3">
<h3>Factor Scores</h3>
<p>In factor analysis, we can obtain estimated factor scores for each observation, possibly to be used in additional analyses or examined in their own right. One common way is to simply use the loadings as one would regression weights/coefficients (actually scaled versions of them), and create a ‘predicted’ factor score as the linear combination of the indicator variables, just as we would to produce predicted values in regression. There is no correct way to do this in standard approaches, as an infinite number of factor scores could be related to the same factor analysis results, a problem known as <span class="emph">factor indeterminancy</span>. However, this doesn’t mean that they aren’t useful, especially if we are employing factor analysis primarily as a data compression technique, where the scores might be used in other analyses (e.g. regression). A common reason to do so is that we might have some SEM model in mind, but too small a sample.</p>
<p>Recent work suggests that different types of factor scores are better used in different roles, e.g. if the factor is to be used as a predictor, the regression/Thurstone method works best. If it is to be used as a dependent variable, then Bartlett factor scores will be unbiased. If using them as <em>both</em>, then adjustments will have to be made to avoid bias. See <a href="http://journals.sagepub.com/doi/abs/10.1177/0013164415607618">Hypothesis Testing Using Factor Score Regression</a> for details (one of the authors is affiliated with lavaan). Results are compared to SEM.</p>
<div id="vs.meanssums" class="section level4">
<h4>vs. Means/Sums</h4>
<p>On many occasions, people reduce the number of variables in a model by using a mean or sum score. These actually can be seen to reflect an underlying factor analysis where all loadings are fixed to be equal and the residual variance of the observed variables is fixed to zero, i.e. perfect measurement. If you really think the items reflect a particular construct, you’d probably be better off using a score that comes from a model that doesn’t assume perfect measurement. I did some simulations that provide more to think about regarding this point, the results of which are <a href="https://m-clark.github.io/docs/lv_sim.html">here</a>.</p>
</div>
<div id="vs.composites" class="section level4">
<h4>vs. Composites</h4>
<p><span class="emph">Composites</span> scores are what we’d have if we turned the arrows around, and allowed different weights for the different variables, which may not be similar too similar in nature or necessarily correlated (e.g. think of how one might construct a measure of socioeconomic status). Unlike a simple mean, these would have different weights associated with the items, which might make them preferred in that comparison. As we noted previously, PCA is one way one could create such a composite. Sometimes people just make up weights to use based on what they think they <em>should</em> be (especially in the sporting world). This is silly in my opinion, as I can’t think of any reasonable justification for such an approach over the many available that would better represent the data.</p>
<!-- <img src="img/composite.png" style="display:block; margin: 0 auto;"></img> -->
<div style="width:250px; margin-left:auto;  margin-right:auto">
<div id="htmlwidget-cebc9f02291203589939" style="width:250px;height:250px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-cebc9f02291203589939">{"x":{"diagram":"digraph composite2 {\n  # Intialization of graph attributes\n  graph [overlap = false bgcolor=transparent]\n\n  # Initialization of node attributes\n  node [shape = circle,\n        fontname = Helvetica,\n        color = gray80,\n        type = box,\n        fixedsize = true]\n\n\n  # Node statements\n  Composite [width=3, height=1, shape=hexagon, color=gray80];\n\n  node [width=1, shape=square, color=gray10]\n  I1; I2; I3; I4; \n\n  # Initialization of edge attributes\n  edge [color = gray50, rel = yields]\n\n  # Edge statements\n  I1 -> Composite [style=dashed]; I2 -> Composite; I3 -> Composite; I4 -> Composite;\n\n  }","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<p>Some also note latent variables with <em>causal</em> indicators, such that the variable is latent, not a composite, but that the arrows go <em>from</em> the indicators as they do with the composite. However, these are not identified by default, and would still require effect indicators to be so (the so-called MIMIC model of multiple indicators and multiple causes). As such, thinking of them this way requires theory adjustment to even do, but there is no inherent reason to think of a variable with a causal effect on a latent variable as we do ‘effect’ indicators, i.e. the normal indicators that are caused <em>by</em> the latent variable, in the first place. Any particular latent variable might be caused any number of observed or other latent variables. That’s just standard SEM though, and doesn’t require a different name or manner of thinking, nor will it have fundamental identification problems. That’s just my opinion though.</p>
</div>
</div>
</div>
<div id="terminology" class="section level2">
<h2>Terminology</h2>
<ul>
<li><strong>Factor Analysis</strong>: The non-PCA factor analytic technique known by various names, but not conducted as SEM, and usually estimated by maximum likelihood.</li>
<li><strong>Latent Variables</strong>, <strong>Factors</strong>, <strong>Constructs</strong> etc.: Terms I use interchangeably.</li>
<li><strong>Item</strong>, <strong>Indicator</strong>, <strong>Observed</strong>, <strong>Manifest</strong>, <strong>Variable</strong>: Terms I use interchangeably.</li>
<li><strong>Loadings</strong>: measures of the relationship between an indicator and the latent variable. For clarity, it’s probably better to use <em>pattern</em> or <em>structure</em> coefficient.</li>
<li><strong>Pattern coefficient</strong>: What is usually displayed in FA results. The path coefficient from a latent variable to some observed variable. In some cases it is a simple correlation coefficient.</li>
<li><strong>Structure coefficient</strong>: The correlation between an observed an latent variable.</li>
<li><strong>Communality</strong>: the amount of variance in the item/variable explained by the (retained) components. It is the sum of the squared loadings.</li>
<li><strong>Uniqueness</strong>: 1 - communality. The unexplained variance of a variable.</li>
<li><strong>Exploratory Factor Analysis</strong>: all factor analysis if done well. By default places no constraints on the loadings of observed variables.</li>
<li><strong>Confirmatory Factor Analysis</strong>: factor analysis done in the SEM context. May in fact not confirm anything. Typically places (perhaps unrealistic) constraints on the model such that some loadings/paths are set to zero.</li>
</ul>
</div>
<div id="some-other-uses-of-latent-variables" class="section level2">
<h2>Some Other Uses of Latent Variables</h2>
<ul>
<li><p><strong>EM algorithm</strong>: A very common technique to estimate model parameters for a variety of model situations, it incorporates a latent variable approach where parameters of interest are treated as a latent variable (e.g. probability of belonging to some cluster).</p></li>
<li><p><strong>Item Response Theory</strong>: uses latent variables, especially in test situations (though is much broader), to assess things like item discrimination, student ability etc. See the related section in this document.</p></li>
<li><p><strong>Hidden Markov Model</strong>: A latent variable model approach commonly used for time series.</p></li>
<li><p><strong>Topic Model</strong>: In the analysis of text, one can discover latent ‘topics’ based on the frequency of words. See the related section in this document.</p></li>
<li><p><strong>Collaborative Filtering</strong>: For example, in recommender systems for movies or music, the latent variable might represent genre or demographic subsets.</p></li>
<li><p><strong>Gaussian Processes</strong>: A common covariance structure in GPs uses a factor analysis distance. I have <a href="https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gp%20Examples/gpStan_squaredExponentialFactorAnalysis.stan">Stan code</a> demonstrating it.</p></li>
<li><p><strong>Multinomial Model</strong> In some more complicated multinomial regression models, a factor analytic structure is used to understand the correlations of coefficients, e.g. across category labels.</p></li>
</ul>
</div>
<div id="summary-3" class="section level2">
<h2>Summary</h2>
<p>Latent variable approaches are a necessary tool to have in your statistical toolbox. Whether your goal is to compress data or explore underlying theoretically motivated constructs, ‘factor-analysis’ will serve you well.</p>
</div>
<div id="r-packages-used-1" class="section level2">
<h2>R packages used</h2>
<ul>
<li><span class="pack">psych</span></li>
<li><span class="pack">lavaan</span></li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="26">
<li id="fn26"><p>Some even use use a factor analytic approach to estimating correlations among parameters in models (e.g. I’ve seen this with gaussian processes and multinomial regression).<a href="latent-variables-1.html#fnref26">↩</a></p></li>
<li id="fn27"><p>Principal components, standard factor analysis and SEM can work on covariance/correlation matrices even without the raw data, this will be perhaps demonstrated in a later version of this doc.<a href="latent-variables-1.html#fnref27">↩</a></p></li>
<li id="fn28"><p>I don’t think it necessary to get into rotation here, though will likely add a bit in the future. If you’re doing PCA, you’re likely not really concerned about interpretation of loadings, as you are going to use the components for other means. It might help with standard factor analysis, but this workshop will spend time on more focused approaches where one would have some idea of the underlying structure rather than looking to uncover the structure. Rotation doesn’t change anything about the fundamental model result, so one just uses whatever leads to the clearest interpretation.<a href="latent-variables-1.html#fnref28">↩</a></p></li>
<li id="fn29"><p>They are the <span class="emph">eigenvalues</span> of the correlation matrix. In addition, they are the diagonal of the crossproduct of the loading matrix.<a href="latent-variables-1.html#fnref29">↩</a></p></li>
<li id="fn30"><p>Many often use what are called <span class="emph">biplots</span> for visualization of PCA, but I find the path diagram more readily understandable.<a href="latent-variables-1.html#fnref30">↩</a></p></li>
<li id="fn31"><p>One version of factor analysis is nearly identical to PCA in terms of mechanics, save for what are on the diagonals of the correlation matrix (1s for PCA vs. ‘communalities’ for FA).<a href="latent-variables-1.html#fnref31">↩</a></p></li>
<li id="fn32"><p>This part of the document borrows notably from the Revelle reference.<a href="latent-variables-1.html#fnref32">↩</a></p></li>
<li id="fn33"><p>For the technically inclined, <span class="math inline">\(W\)</span> are the eigenvectors of <span class="math inline">\(X&#39;X\)</span>, or if the data is standardized, this is the same as the eigenvector of the covariance/correlation matrix of <span class="math inline">\(X\)</span>. Technically, <span class="pack">psych</span> uses weights based on the loadings and covariance matrix via <code>solve(cov(bfi_trim), pc$loadings)</code>, which amounts to an equivalent result (even if different in sign/value). For factor analysis this becomes more nuanced. There is no unique way to create a factor score, but the common ones use weights that are a function of the loadings and correlation matrix. For example, the most commonly used (‘Thurstone’ or ‘regression’) are constructed via <span class="math inline">\(V = R^{-1}W\)</span>, where <span class="math inline">\(R\)</span> is the correlation matrix and <span class="math inline">\(W\)</span> are the loadings as before.<a href="latent-variables-1.html#fnref33">↩</a></p></li>
<li id="fn34"><p>For those that might wonder, I use the ‘gls’ method here only to get the result for demonstration purposes. A five factor model with five variables is not identified (nor is a 4 or 3 factor model).<a href="latent-variables-1.html#fnref34">↩</a></p></li>
<li id="fn35"><p>Unfortunately for users of SPSS, the default setting for ‘factor analysis’ in the menus is actually PCA, and this has lead to erroneous reporting of results for decades where people thought they had conducted factor analysis but in fact had not. I’ll bite my tongue and just say that this is one of many ridiculous defaults that SPSS has.<a href="latent-variables-1.html#fnref35">↩</a></p></li>
<li id="fn36"><p>There are actually deep philosophical underpinnings to this approach, going at least as far back as the notion of the Platonic forms, and continuing on through philosophical debates about what mental aspects can be measured scientifically. However, even when it became a more quantitative discipline, the philosophy was not far behind. See, for example, <a href="https://archive.org/details/vectorsofmindmul010122mbp">The Vectors of Mind</a> by L.L. Thurstone, one of the pioneers of measurement theory (1935). As a philosophy major from back in the day, latent variable modeling has always had great appeal to me.<a href="latent-variables-1.html#fnref36">↩</a></p></li>
<li id="fn37"><p>For your own sake, if you develop a questionnaire, make higher numeric values correspond to meaning ‘more of’ something, rather than in this backward fashion. You’re basically begging for a less reliable measure otherwise.<a href="latent-variables-1.html#fnref37">↩</a></p></li>
<li id="fn38"><p>Note that this is actually done for all disturbance/residual terms, as there is an underlying latent variable there which represents measurement error and the effect of unexplained causes. The path of that latent variable is fixed to 1, and its variance is the residual variance in the SEM output.<a href="latent-variables-1.html#fnref38">↩</a></p></li>
<li id="fn39"><p>Kline distinguishes between the residuals in standard regression and disturbance in SEM (p. 131 4th ed.), but the distinction there appears to conflate the estimated variance as a parameter/construct and the actual residuals (<span class="math inline">\(y - \hat{y}\)</span>) you’d get after model estimation. A standard regression as typically estimated is no different than the same model in the graphical modeling context. Calling path analysis a causal model makes it no more causal than any other regression model, and the remaining variance is the effect of many things not in the model, and they are causal, regardless of estimation technique. I think we care more deeply about it in the SEM context, and perhaps that necessitates another name, and anything would be better than ‘error’.<a href="latent-variables-1.html#fnref39">↩</a></p></li>
<li id="fn40"><p>Note that this is similar to the vast majority of psychological research, which is typically conducted on what isn’t even a random sample of college attendees.<a href="latent-variables-1.html#fnref40">↩</a></p></li>
<li id="fn41"><p>The Minnesota Multiphasic Personality Inventory, or MMPI, was left unchanged for <em>45 years</em> before a second version, the MMPI-2, came along in 1989. Since then one more has been released, the MMPI-2-RF. That’s like going from version 1 to 2 to 2.5 in 65 years (plus the almost 10 since the last version). Other versions have been constructed for adolescents (soon to be updated for the first time since 1992), and other languages. This slow development has occurred despite known issues for certain segments of the population, to which it continues to be applied.<a href="latent-variables-1.html#fnref41">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="graphical-models-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="structural-equation-modeling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "san-serif",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "chapter",
"depth": 3
},
"highlight": "pygments",
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
